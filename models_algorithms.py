# -*- coding: utf-8 -*-
"""MoNuSeg_nuclei_segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4tF7LeoNPvdOt-YQmJS7EsAzDO0Y3zX
"""

from google.colab import drive
drive.mount('/content/drive')
!pip install kaggle
from google.colab import files
files.upload()
!mkdir -p /root/.kaggle/
!mv /content/kaggle.json /root/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json
!kaggle datasets download -d tuanledinh/processedoriginalmonuseg -p /content

!unzip /content/processedoriginalmonuseg.zip -d data

!cp /content/drive/MyDrive/4-MonuseG/weights/deep_unet_final_weights.h5 /content

from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import History
import time
import pandas as pd
import pickle
import tensorflow as tf
import gc
from keras.optimizers import Adam

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy

import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras.layers as tfl

image_path = '/content/data/processed-original-monuseg/train_folder/img/0001.png'
img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)

img_array /= 255.0

activations = model.predict(img_array)

label_path = '/content/data/processed-original-monuseg/train_folder/labelcol/0001.png'
label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)
label = label / 255.0  # Нормализация до диапазона [0, 1]

threshold = 0.5
predicted_mask = activations[0, :, :, 0] > threshold

true_positive = np.logical_and(predicted_mask, label)
false_positive = np.logical_and(predicted_mask, np.logical_not(label))
false_negative = np.logical_and(np.logical_not(predicted_mask), label)

alpha_channel = np.zeros_like(predicted_mask, dtype=np.uint8)
alpha_channel[predicted_mask] = 255

# Создаем изображение с альфа-каналом для overlay
overlay = img_array[0].copy()
overlay_color = [0, 0, 255]  # RGB код для синего цвета
overlay[alpha_channel == 255] = overlay_color

# Визуализация различий
plt.figure(figsize=(20, 20))

# Выводим оригинальное изображение
plt.subplot(1, 4, 1)
plt.title('Original Image')
plt.imshow(img_array[0])
plt.axis('off')

# Выводим истинную сегментацию
plt.subplot(1, 4, 2)
plt.title('True Segmentation')
plt.imshow(label, cmap='gray')
plt.axis('off')

colored_diff = np.zeros_like(img_array[0])

colored_diff[false_positive] = [255, 0, 0]

colored_diff[false_negative] = [0, 255, 0]

colored_diff[true_positive] = [0, 0, 255]

plt.subplot(1, 4, 3)
plt.title('Predicted Segmentation with Differences')
plt.imshow(colored_diff)
plt.axis('off')

plt.subplot(1, 4, 4)
plt.title('Predicted Segmentation')
plt.imshow(overlay)
plt.axis('off')

plt.show()

"""**Augmentation**"""

import os
import cv2
import numpy as np
import albumentations as A
from tensorflow.keras.preprocessing.image import img_to_array, load_img
import matplotlib.colors as mcolors

# Установите желаемый размер батча
BATCH_SIZE = 8

# Путь к папкам с изображениями и масками
folders = [
    "/content/data/processed-original-monuseg/train_folder/img",
    "/content/data/processed-original-monuseg/train_folder/labelcol"
]

# Размеры, к которым мы хотим привести изображения и маски
desired_size = (256, 256)

# Создадим папки для сохранения измененных данных
resized_data_dir = '/content/resized_data'
os.makedirs(resized_data_dir, exist_ok=True)

def custom_augmentation(image, mask):
    # Создаем экземпляр аугментации
    transform = A.Compose([
        A.RandomBrightnessContrast(brightness_limit=(0.04, 0.01), contrast_limit=(0.01, 0.06), p=0.01),  # Увеличиваем яркость случайным образом
        A.GaussianBlur(p=0.2),  # Вы можете закомментировать это, если не нужно размывание
        A.RandomRotate90(p=0.5),
        A.RandomBrightnessContrast(brightness_limit=(-0.03, 0.03), contrast_limit=(-0.03, 0.03), p=1),
        #A.RandomFog(p=0.06),  #
        #A.HorizontalFlip(p=0.5),
        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=45, p=0.5),

        A.RGBShift(r_shift_limit=0.06, g_shift_limit=0.07, b_shift_limit=0.05, p=0.5),
        # A.MedianBlur(p=0.05, blur_limit=(3, 5)),
        A.Rotate(p=0.5, limit=(-90, 90))
    ])

    # Применяем аугментацию к изображению и маске
    augmented = transform(image=image, mask=mask)
    augmented_image = augmented['image']
    augmented_mask = augmented['mask']


    # augmented_mask = cv2.GaussianBlur(augmented_mask, (3, 3), 0)  # Здесь (5, 5) - размер ядра размытия, можно изменить по желанию

    return augmented_image, augmented_mask

# Функция для изменения размера изображения и маски
def resize_image_and_mask(image_path, mask_path, target_size):
    image = cv2.imread(image_path)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    # Изменение размера изображения
    resized_image = cv2.resize(image, target_size)

    # Изменение размера маски и преобразование ее в формат (256, 256, 1)
    resized_mask = cv2.resize(mask, target_size)
    resized_mask = np.expand_dims(resized_mask, axis=-1)
    mask = mask.reshape(mask.shape + (1,))
    return resized_image, resized_mask

# Функция для загрузки и аугментации данных
def load_and_preprocess_data(images_dir, masks_dir, target_size, batch_size):
    # Получаем список путей к изображениям и маскам
    image_paths = [os.path.join(images_dir, filename) for filename in os.listdir(images_dir) if filename.endswith('.png')]
    mask_paths = [os.path.join(masks_dir, filename) for filename in os.listdir(masks_dir) if filename.endswith('.png')]

    # Загружаем и преобразуем изображения и маски
    images = [img_to_array(load_img(path, target_size=target_size, color_mode='rgb')) for path in image_paths]
    masks = [cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in mask_paths]
    masks = [np.expand_dims(mask, axis=-1) for mask in masks]  # Преобразуем маски в формат (256, 256, 1)

    images = np.array(images) / 255.0
    masks = np.array(masks) / 255.0  # Преобразуем маски в диапазон [0, 1]

    augmented_images = []
    augmented_masks = []

    for i in range(len(images)):
        image = images[i]
        mask = masks[i]

        augmented_image, augmented_mask = custom_augmentation(image, mask)

        augmented_images.append(augmented_image)
        augmented_masks.append(augmented_mask)

    augmented_images = np.array(augmented_images)
    augmented_masks = np.array(augmented_masks)

    desired_total_samples = 2000
    current_samples = len(augmented_images)

    while len(augmented_images) < desired_total_samples:
        if current_samples > 0:
            random_index = np.random.randint(0, current_samples)

            image = augmented_images[random_index]
            mask = augmented_masks[random_index]

            augmented_image, augmented_mask = custom_augmentation(image, mask)

            augmented_images = np.append(augmented_images, [augmented_image], axis=0)
            augmented_masks = np.append(augmented_masks, [augmented_mask], axis=0)
        else:
            break

    # Обрезаем значения пикселей в пределах [0, 1]
    augmented_images = np.clip(augmented_images, 0.0, 1.0)

    # Сохраняем сгенерированные данные в отдельных папках
    augmented_images_dir = os.path.join(resized_data_dir, 'images')
    augmented_masks_dir = os.path.join(resized_data_dir, 'masks')
    os.makedirs(augmented_images_dir, exist_ok=True)
    os.makedirs(augmented_masks_dir, exist_ok=True)

    for i, (image, mask) in enumerate(zip(augmented_images, augmented_masks)):
        image_filename = f'image_{i}.png'
        mask_filename = f'mask_{i}.png'
        image_path = os.path.join(augmented_images_dir, image_filename)
        mask_path = os.path.join(augmented_masks_dir, mask_filename)

        cv2.imwrite(image_path, (image * 255).astype(np.uint8))
        cv2.imwrite(mask_path, (augmented_mask * 255).astype(np.uint8))  # Сохраняем яркие маски

    return augmented_images, augmented_masks

# Загрузка и обработка данных
for i in range(0, len(folders), 2):
    images, masks = load_and_preprocess_data(folders[i], folders[i + 1], desired_size, BATCH_SIZE)

    # Вывод информации о размерах данных
    print(f"Number of images in {folders[i]}:", len(images))
    print(f"Number of masks in {folders[i + 1]}:", len(masks))
    print(f"Shape of images in {folders[i]}:", images.shape)
    print(f"Shape of masks in {folders[i + 1]}:", masks.shape)

import os
import cv2
import matplotlib.pyplot as plt
import math
import matplotlib.colors as mcolors


mask_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

def visualize_augmented_images_and_masks(images, masks, num_samples=5, mask_cmap=None):
    num_samples = min(num_samples, len(images))
    sample_indices = range(num_samples)  # Вместо random.sample берем первые num_samples элементов

    for i, idx in enumerate(sample_indices):
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # Визуализация аугментированного изображения
        augmented_image = images[idx]
        axes[0].imshow(augmented_image)
        axes[0].set_title(f"Augmented Image {i + 1}")
        axes[0].axis('off')

        # Визуализация аугментированной маски с цветовой картой
        augmented_mask = masks[idx]
        if mask_cmap:
            axes[1].imshow(augmented_mask.squeeze(), cmap=mask_cmap)
        else:
            axes[1].imshow(augmented_mask.squeeze(), cmap='gray')
        axes[1].set_title(f"Augmented Mask {i + 1}")
        axes[1].axis('off')

        plt.show()

# Визуализация аугментированных изображений и масок с использованием цветовой карты
visualize_augmented_images_and_masks(images[:100], masks[:100], num_samples=30, mask_cmap=mask_cmap)

# Шаг 2: Разделение на обучающую, валидационную и тестовую выборки
def split_data(images, masks, train_percent, val_percent, test_percent):
    total_samples = len(images)
    num_train = int(total_samples * train_percent)
    num_val = int(total_samples * val_percent)

    train_images, remaining_images = images[:num_train], images[num_train:]
    train_masks, remaining_masks = masks[:num_train], masks[num_train:]

    val_images, test_images = remaining_images[:num_val], remaining_images[num_val:]
    val_masks, test_masks = remaining_masks[:num_val], remaining_masks[num_val:]

    return (train_images, train_masks), (val_images, val_masks), (test_images, test_masks)

# Определяем проценты для разделения
train_percent = 0.8
val_percent = 0.10
test_percent = 0.10

# Разделяем данные на обучающую, валидационную и тестовую выборки
(train_images, train_masks), (val_images, val_masks), (test_images, test_masks) = split_data(images, masks, train_percent, val_percent, test_percent)

# Выводим размеры полученных выборок
print("Number of training samples:", len(train_images))
print("Number of validation samples:", len(val_images))
print("Number of test samples:", len(test_images))

import tensorflow as tf
# Шаг 3: Создание итераторов данных
def create_data_generators(train_images, train_masks, val_images, val_masks, test_images, test_masks, batch_size):
    train_data = tf.data.Dataset.from_tensor_slices((train_images, train_masks))
    val_data = tf.data.Dataset.from_tensor_slices((val_images, val_masks))
    test_data = tf.data.Dataset.from_tensor_slices((test_images, test_masks))

    train_data = train_data.shuffle(len(train_images)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    val_data = val_data.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    test_data = test_data.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

    return train_data, val_data, test_data

# Создаем итераторы данных
train_data, val_data, test_data = create_data_generators(train_images, train_masks, val_images, val_masks, test_images, test_masks, BATCH_SIZE)

import tensorflow.keras.backend as K
from scipy.spatial.distance import directed_hausdorff

from tensorflow.keras.losses import BinaryCrossentropy

# Функция Dice Coefficient
def dice_coef(y_true, y_pred):
    smooth = 1.0
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2.0 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

# Функция F1 Score
def f1_score(y_true, y_pred):
    precision_val = precision(y_true, y_pred)
    recall_val = recall(y_true, y_pred)
    return 2.0 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))

# Функция Mean IoU (Intersection over Union)
def mean_iou(y_true, y_pred):
    intersection = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    union = K.sum(K.round(K.clip(y_true + y_pred, 0, 1)))
    iou = intersection / (union + K.epsilon())
    return iou

# Функции точности (Accuracy), полноты (Recall) и точности (Precision)
def accuracy(y_true, y_pred):
    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)

def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall_val = true_positives / (possible_positives + K.epsilon())
    return recall_val

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision_val = true_positives / (predicted_positives + K.epsilon())
    return precision_val

def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    sensitivity_val = true_positives / (possible_positives + K.epsilon())
    return sensitivity_val

"""**UNet_base**"""

from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate
from tensorflow.keras.models import Model

# Входной тензор
input_shape = (256, 256, 3)
inputs = Input(input_shape)

# Энкодер
conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

# Боттом-блок
conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)

# Декодер
up5 = UpSampling2D(size=(2, 2))(conv4)
up5 = Concatenate()([up5, conv3])
conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)
conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)

up6 = UpSampling2D(size=(2, 2))(conv5)
up6 = Concatenate()([up6, conv2])
conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)
conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)

up7 = UpSampling2D(size=(2, 2))(conv6)
up7 = Concatenate()([up7, conv1])
conv7 = Conv2D(64, 3, activation='relu', padding='same')(up7)
conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)

# Выходной слой
output = Conv2D(1, 1, activation='sigmoid')(conv7)

# Создание модели
base_unet = Model(inputs=inputs, outputs=output)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
base_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

# Выводим информацию о модели
base_unet.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("base_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_base_unet = base_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
base_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_base_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("base_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("base_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_base_unet.history, history_file)

# Save the final trained weights
base_unet.save_weights("base_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_base_unet.history['loss'], label='Training Loss')
plt.plot(history_base_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_base_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_base_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_base_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_base_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_base_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_base_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_base_unet.history['precision'], label='Training Precision')
plt.plot(history_base_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_base_unet.history['recall'], label='Training Recall')
plt.plot(history_base_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

test_metrics = base_unet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(base_unet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

"""**DEEp_UNet**"""

import tensorflow as tf
import tensorflow.keras.layers as tfl
from keras.optimizers import Adam


def conv_block(inputs=None, n_filters=64, dropout_prob=0.0, max_pooling=True):
    conv = tfl.Conv2D(n_filters,  # Number of filters
                      3,  # Kernel size
                      padding='same',
                      kernel_initializer='he_normal')(inputs)
    conv = tfl.BatchNormalization()(conv)
    conv = tfl.Activation('relu')(conv)
    conv = tfl.Conv2D(n_filters,  # Number of filters
                      3,  # Kernel size
                      padding='same',
                      kernel_initializer='he_normal')(conv)
    conv = tfl.BatchNormalization()(conv)
    conv = tfl.Activation('relu')(conv)

    if dropout_prob > 0:
        conv = tfl.Dropout(dropout_prob)(conv)

    if max_pooling:
        next_layer = tfl.MaxPool2D(
            pool_size=(2, 2), strides=2)(conv)
    else:
        next_layer = conv
    skip_connection = conv
    return next_layer, skip_connection

def upsampling_block(expansive_input, contractive_input, n_filters=64):
    up = tfl.Conv2DTranspose(
        n_filters,  # number of filters
        3,  # Kernel size
        strides=2,
        padding='same')(expansive_input)

    merge = tfl.concatenate([up, contractive_input], axis=3)
    conv = tfl.Conv2D(n_filters,  # Number of filters
                      3,  # Kernel size
                      padding='same',
                      kernel_initializer='he_normal')(merge)
    conv = tfl.BatchNormalization()(conv)
    conv = tfl.Activation('relu')(conv)
    conv = tfl.Conv2D(n_filters,  # Number of filters
                      3,  # Kernel size
                      padding='same',
                      kernel_initializer='he_normal')(conv)
    conv = tfl.BatchNormalization()(conv)
    conv = tfl.Activation('relu')(conv)
    return conv

def create_deep_unet(image_size, dropout_prob):
    n_filters = 64
    input_size = image_size + (3,)
    inputs = tfl.Input(input_size)
    cblock1 = conv_block(inputs, n_filters)
    cblock2 = conv_block(cblock1[0], n_filters * 2)
    cblock3 = conv_block(cblock2[0], n_filters * 4)
    cblock4 = conv_block(cblock3[0], n_filters * 8, dropout_prob=dropout_prob)
    cblock5 = conv_block(cblock4[0], n_filters * 16,
                         dropout_prob=dropout_prob, max_pooling=False)
    ublock6 = upsampling_block(cblock5[0], cblock4[1], n_filters * 8)
    ublock7 = upsampling_block(ublock6, cblock3[1], n_filters * 4)
    ublock8 = upsampling_block(ublock7, cblock2[1], n_filters * 2)
    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters)

    conv9 = tfl.Conv2D(n_filters,
                       3,
                       activation='relu',
                       padding='same',
                       kernel_initializer='he_normal')(ublock9)
    if dropout_prob > 0.0:
        conv9 = tfl.Dropout(dropout_prob)(conv9)
    conv10 = tfl.Conv2D(1, 1, activation='sigmoid', padding='same')(conv9)
    deep_unet = tf.keras.Model(inputs=inputs, outputs=conv10)
    return deep_unet

dropout_prob = 0.1
input_size = (256, 256)  # Уберите третью размерность (количество каналов)
deep_unet = create_deep_unet(input_size, dropout_prob)
deep_unet.summary()

optimizer = Adam(learning_rate=1e-4)
loss = BinaryCrossentropy()

deep_unet.compile(optimizer=optimizer,
                   loss=loss,
                   metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("deep_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_deep_unet = deep_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=2,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
deep_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_deep_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("deep_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("deep_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_deep_unet.history, history_file)

# Save the final trained weights
deep_unet.save_weights("deep_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_deep_unet.history['loss'], label='Training Loss')
plt.plot(history_deep_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_deep_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_deep_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_deep_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_deep_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_deep_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_deep_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_deep_unet.history['precision'], label='Training Precision')
plt.plot(history_deep_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_deep_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_deep_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_deep_unet.history['recall'], label='Training Recall')
plt.plot(history_deep_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

test_metrics = deep_unet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(deep_unet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

import gc

del deep_unet
gc.collect()

"""UNet CBam"""

import tensorflow as tf
from tensorflow.keras.layers import Add, Dense, Conv2D, Concatenate, MaxPooling2D, Activation, GlobalAveragePooling2D, GlobalMaxPooling2D
from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D, Dropout, BatchNormalization, Reshape, multiply
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Lambda, Permute, Concatenate

'''
U-Net: Convolutional Networks for Biomedical Image Segmentation
(https://arxiv.org/abs/1505.04597)
CBAM: Convolutional Block Attention Module
(https://arxiv.org/pdf/1807.06521.pdf)
---
img_shape: (height, width, channels)
out_ch: number of output channels
start_ch: number of channels of the first conv
depth: zero indexed depth of the U-structure
inc_rate: rate at which the conv channels will increase
activation: activation function after convolutions
dropout: amount of dropout in the contracting part
res_rate: rate at which the residual blocks repeat
ratio: ratio of MLP with one hidden layer
batchnorm: adds Batch Normalization if true
maxpool: use strided conv instead of maxpooling if false
upconv: use transposed conv instead of upsamping + conv if false
residual: add residual connections around each attention block if true
dilated: change convolution to dilate convolution for fast train if true
'''


def get_unetcbam_model(input_channel_num=3, out_ch=3, start_ch=16, depth=5, inc_rate=2., activation='relu',
                       dropout=0.5, ratio=4, batchnorm=True, maxpool=False, upconv=True, residual=False, dilated=False):
    def _channel_attention(m, out_ch, ratio):
        avg_pool = GlobalAveragePooling2D()(m)
        avg_pool = Reshape((1, 1, out_ch))(avg_pool)
        avg_pool = Dense(units=out_ch // ratio)(avg_pool)
        avg_pool = Activation('relu')(avg_pool)
        avg_pool = Dense(units=out_ch)(avg_pool)

        max_pool = GlobalMaxPooling2D()(m)
        max_pool = Reshape((1, 1, out_ch))(max_pool)
        max_pool = Dense(units=out_ch // ratio)(max_pool)
        max_pool = Activation('relu')(max_pool)
        max_pool = Dense(units=out_ch)(max_pool)

        channel_attention = Add()([avg_pool, max_pool])
        channel_attention = Activation('sigmoid')(channel_attention)

        return multiply([m, channel_attention])

    def _spatial_attention(m, out_ch, ratio, kernel_size=7):
        if tf.keras.backend.image_data_format() == "channels_first":
            spatial_attention = Permute((2, 3, 1))(m)
        else:
            spatial_attention = m

        avg_pool = Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True))(spatial_attention)
        max_pool = Lambda(lambda x: tf.keras.backend.max(x, axis=3, keepdims=True))(spatial_attention)
        concat = Concatenate(axis=3)([avg_pool, max_pool])
        spatial_attention = Conv2D(1, kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal',
                                   use_bias=False)(concat)

        if tf.keras.backend.image_data_format() == "channels_first":
            spatial_attention = Permute((3, 1, 2))(spatial_attention)

        return multiply([m, spatial_attention])

    def _conv_block(m, dim, acti, bn, di, res, do=0):
        n = Conv2D(dim, 3, padding='same', dilation_rate=2)(m) if di else Conv2D(dim, 3, padding='same')(m)
        n = BatchNormalization()(n) if bn else n
        n = Activation(acti)(n)
        n = Dropout(do)(n) if do else n
        n = Conv2D(dim, 3, padding='same', dilation_rate=2)(n) if di else Conv2D(dim, 3, padding='same')(n)
        n = BatchNormalization()(n) if bn else n
        n = Activation(acti)(n)
        n = Add()([n, _channel_attention(n, dim, ratio)]) if res else _channel_attention(n, dim, ratio)
        n = Add()([n, _spatial_attention(n, dim, ratio)]) if res else _spatial_attention(n, dim, ratio)

        return n

    def _level_block(m, dim, depth, inc, acti, do, bn, mp, up, res, di):
        if depth > 0:
            n = _conv_block(m, dim, acti, bn, di, res)
            m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)
            m = _level_block(m, int(inc * dim), depth - 1, inc, acti, do, bn, mp, up, res, di)
            if up:
                m = UpSampling2D()(m)
                m = Conv2D(dim, 2, activation=acti, padding='same')(m)
            else:
                m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)
            n = Concatenate()([n, m])
            m = _conv_block(n, dim, acti, bn, di, res)
        else:
            m = _conv_block(m, dim, acti, bn, di, res, do)

        return m

    i = Input(shape=(None, None, input_channel_num))
    o = _level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual, dilated)
    o = Conv2D(1, 1, activation='sigmoid')(o)

    model = Model(inputs=i, outputs=o)

    return model

# Компилируйте модель
optimizer = Adam(learning_rate=0.001)  # Выберите желаемую скорость обучения
unetcbam_model = get_unetcbam_model()

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
unetcbam_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

# Выведите информацию о модели
unetcbam_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("unetcbam_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_unetcbam_model = unetcbam_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=2,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
unetcbam_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_unetcbam_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("unetcbam_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("unetcbam_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_unetcbam_model.history, history_file)

# Save the final trained weights
unetcbam_model.save_weights("unetcbam_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_unetcbam_model.history['loss'], label='Training Loss')
plt.plot(history_unetcbam_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_unetcbam_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_unetcbam_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unetcbam_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unetcbam_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_unetcbam_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_unetcbam_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_unetcbam_model.history['precision'], label='Training Precision')
plt.plot(history_unetcbam_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unetcbam_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unetcbam_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_unetcbam_model.history['recall'], label='Training Recall')
plt.plot(history_unetcbam_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

test_metrics = unetcbam_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(unetcbam_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

import gc

del unetcbam_model
gc.collect()

"""**nuclei segnet**"""

import tensorflow as tf
from tensorflow.keras.activations import relu
from tensorflow.keras.initializers import Constant
from tensorflow.keras.models import Model
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import *
from tensorflow.keras.activations import sigmoid
from tensorflow.keras.layers import SeparableConv2D
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Reshape

## Crop and Merge Layers ##
def CropAndMerge(Input1, Input2):
    """
    Crop input1 so that it matches input2 and then
    return the concatenation of both channels.
    """
    Size1_x = Input1.shape[1]
    Size2_x = Input2.shape[1]
    Size1_y = Input1.shape[2]
    Size2_y = Input2.shape[2]
    diff_x = tf.divide(tf.subtract(Size1_x, Size2_x), 2)
    diff_y = tf.divide(tf.subtract(Size1_y, Size2_y), 2)
    diff_x = tf.cast(diff_x, tf.int32)
    Size2_x = tf.cast(Size2_x, tf.int32)
    diff_y = tf.cast(diff_y, tf.int32)
    Size2_y = tf.cast(Size2_y, tf.int32)
    crop = tf.slice(Input1, [0, diff_x, diff_y, 0], [-1, Size2_x, Size2_y, -1])
    concat = tf.concat([crop, Input2], axis=3)
    return concat

## Attention Mechanism ##
def attention_gt(input_x, input_g, fil_las):
    input_size = input_x.shape
    fil_int = fil_las // 2

    input_g = Conv2D(filters=fil_las, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding='same', use_bias=True,
        kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input_g)

    theta_x = Conv2D(filters=fil_int, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,
                  kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input_x)

    theta_x_size = theta_x.shape

    phi_g = Conv2D(filters=fil_int, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,
              kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input_g)

    phi_g_u = UpSampling2D(size=(2, 2), interpolation='bilinear')(phi_g)
    f = relu(add([theta_x, phi_g_u]))

    psi_f = Conv2D(filters=fil_las, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding='same', use_bias=True,
              kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(f)

    psi_f = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0), gamma_initializer=Constant(1.0), momentum=0.5)(psi_f)

    sigm_psi_f = sigmoid(psi_f)

    expand = Reshape(target_shape=input_size[-3:])(sigm_psi_f)

    y = multiply([expand, input_x])

    return y

## Conv Block
def conv_block(input, filters):
    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0), gamma_initializer=Constant(1.0), momentum=0.5)(x)
    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(x)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0), gamma_initializer=Constant(1.0), momentum=0.5)(x)
    return x

## Bottleneck Block
def bottleneck_block(input, filters):
    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,
              kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0),
                          gamma_initializer=Constant(1.0), momentum=0.5)(x)
    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,
              kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(x)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0),
                          gamma_initializer=Constant(1.0), momentum=0.5)(x)
    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True,
              kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(x)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0),
                          gamma_initializer=Constant(1.0), momentum=0.5)(x)
    return x

## Robust Residual block
def robust_residual_block(input, filters_inp):
    x1 = Conv2D(filters=filters_inp, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',
                use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input)
    x1 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01),
                            gamma_initializer=Constant(1.0), momentum=0.5)(x1)
    x2 = SeparableConv2D(filters=filters_inp, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same',
                        use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform',
                        kernel_initializer='glorot_normal', bias_initializer=Constant(0.1), depth_multiplier=1)(x1)
    x2 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01),
                            gamma_initializer=Constant(1.0), momentum=0.5)(x2)
    x3 = Conv2D(filters=filters_inp, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',
                use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(x2)
    x3 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01),
                            gamma_initializer=Constant(1.0), momentum=0.5)(x3)
    x = concatenate([input, x3], axis=-1)
    x = Conv2D(filters=filters_inp, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(x)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01),
                          gamma_initializer=Constant(1.0), momentum=0.5)(x)
    return x

## Attention Block ##
def attention_decoder_block(input, filt, conc):
    atten_b = attention_gt(input_x=conc, input_g=input, fil_las=filt)
    x_ct = Conv2DTranspose(filters=filt, kernel_size=(2, 2), activation='relu', strides=(2, 2), padding='same', kernel_initializer='glorot_normal', bias_initializer=Constant(0.1), use_bias=True)(input)
    x = CropAndMerge(Input1=x_ct, Input2=atten_b)
    return x

def nuclei_segnet(input_shape, num_classes=1, output_activation='sigmoid'):
    inputs = Input(input_shape)
    filters = 32

    x_conv1 = robust_residual_block(inputs, filters)
    x_pool1 = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(x_conv1)
    x_conv2 = robust_residual_block(x_pool1, filters * 2)
    x_pool2 = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(x_conv2)
    x_conv3 = robust_residual_block(x_pool2, filters * 4)
    x_pool3 = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(x_conv3)
    x_conv4 = robust_residual_block(x_pool3, filters * 8)
    x_pool4 = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(x_conv4)
    x_conv5 = bottleneck_block(x_pool4, filters * 16)

    x_tconv5 = attention_decoder_block(x_conv5, filters * 8, x_conv4)
    u_conv4 = conv_block(x_tconv5, filters * 8)
    x_tconv4 = attention_decoder_block(u_conv4, filters * 4, x_conv3)
    u_conv3 = conv_block(x_tconv4, filters * 4)
    x_tconv3 = attention_decoder_block(u_conv3, filters * 2, x_conv2)
    u_conv2 = conv_block(x_tconv3, filters * 2)
    x_tconv2 = attention_decoder_block(u_conv2, filters, x_conv1)
    u_conv1 = conv_block(x_tconv2, filters)

    outputs = Conv2D(num_classes, (1, 1), activation=output_activation, padding='same')(u_conv1)

    nuclei_segnet = Model(inputs=inputs, outputs=outputs)

    # Define your optimizer and loss function
    optimizer = Adam(learning_rate=0.001)
    loss = BinaryCrossentropy()

    nuclei_segnet.compile(optimizer=optimizer,
               loss=loss,
               metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

    return nuclei_segnet

input_shape = (256, 256, 3)

# Train the model
nuclei_segnet = nuclei_segnet(input_shape, num_classes=1, output_activation='sigmoid')

num_epochs = 160
batch_size = 8

nuclei_segnet.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("nuclei_segnet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_nuclei_segnet = nuclei_segnet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=2,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
nuclei_segnet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_nuclei_segnet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("nuclei_segnet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("nuclei_segnet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_nuclei_segnet.history, history_file)

# Save the final trained weights
nuclei_segnet.save_weights("nuclei_segnet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_nuclei_segnet.history['loss'], label='Training Loss')
plt.plot(history_nuclei_segnet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_nuclei_segnet.history['accuracy'], label='Training Accuracy')
plt.plot(history_nuclei_segnet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_nuclei_segnet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_nuclei_segnet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_nuclei_segnet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_nuclei_segnet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_nuclei_segnet.history['precision'], label='Training Precision')
plt.plot(history_nuclei_segnet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_nuclei_segnet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_nuclei_segnet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_nuclei_segnet.history['recall'], label='Training Recall')
plt.plot(history_nuclei_segnet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

test_metrics = nuclei_segnet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(nuclei_segnet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del nuclei_segnet
gc.collect()

"""**SA_Net**"""

from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Activation, multiply, add, Dropout, BatchNormalization, SpatialDropout2D
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras import regularizers

def SalientAttentionBlock(f_maps, sal_ins, pool_maps, num_fmaps, dropout_rate):
    conv1_salins = Conv2D(128, (1, 1), activation='relu')(sal_ins)
    conv1_salins = BatchNormalization()(conv1_salins)

    conv1_fmaps = Conv2D(128, (1, 1), strides=(2, 2), activation='relu')(f_maps)
    conv1_fmaps = BatchNormalization()(conv1_fmaps)

    attn_add = add([conv1_fmaps, conv1_salins])

    conv_1d = Conv2D(128, (3, 3), activation='relu', padding='same')(attn_add)
    conv_1d = BatchNormalization()(conv_1d)

    conv_1d = Conv2D(128, (3, 3), activation='relu', padding='same')(conv_1d)
    conv_1d = BatchNormalization()(conv_1d)

    conv_1d = Conv2D(1, (1, 1), activation='relu')(conv_1d)
    conv_1d = BatchNormalization()(conv_1d)

    conv_nd = Conv2D(num_fmaps, (1, 1), activation='relu')(conv_1d)
    attn_act = Activation('sigmoid')(conv_nd)

    attn = multiply([attn_act, pool_maps])
    attn = BatchNormalization()(attn)

    return attn

def UNetBlock(in_fmaps, num_fmaps, dropout_rate, regularization_coeff=1e-4):
    conv1 = Conv2D(num_fmaps, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(regularization_coeff))(in_fmaps)
    conv1 = BatchNormalization()(conv1)

    conv1 = SpatialDropout2D(dropout_rate)(conv1)

    conv_out = Conv2D(num_fmaps, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(regularization_coeff))(conv1)
    conv_out = BatchNormalization()(conv_out)

    return conv_out

def Network(input_size=(256, 256, 3), dropout_rate=0.5):
    input1 = Input(shape=input_size)


    conv1 = UNetBlock(input1, 32, dropout_rate)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)



    conv2 = UNetBlock(pool1, 32, dropout_rate)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)



    conv3 = UNetBlock(pool2, 64, dropout_rate)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)



    conv4 = UNetBlock(pool3, 64, dropout_rate)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    #

    conv5 = UNetBlock(pool4, 128, dropout_rate)

    up6 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv5), pool3], axis=3)
    conv6 = UNetBlock(up6, 64, dropout_rate)

    up7 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6), pool2], axis=3)
    conv7 = UNetBlock(up7, 64, dropout_rate)

    up8 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7), pool1], axis=3)
    conv8 = UNetBlock(up8, 32, dropout_rate)

    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)
    conv9 = UNetBlock(up9, 32, dropout_rate)

    # Дополнительные блоки UNetBlock
    conv10 = UNetBlock(conv9, 32, dropout_rate)
    conv11 = UNetBlock(conv10, 32, dropout_rate)
    conv12 = UNetBlock(conv11, 32, dropout_rate)

    # Три дополнительных блока UNetBlock
    conv13 = UNetBlock(conv12, 32, dropout_rate)
    conv14 = UNetBlock(conv13, 32, dropout_rate)
    conv15 = UNetBlock(conv14, 32, dropout_rate)

    conv_final = Conv2D(1, (1, 1), activation='sigmoid')(conv15)  # Финальный слой

    model = Model(inputs=input1, outputs=conv_final)

    return model

# Создаем модель с новыми размерами, слоями Dropout, BatchNormalization и регуляризацией
input_size = (256, 256, 3)
dropout_rate = 0.5
sa_unet = Network(input_size, dropout_rate)
sa_unet.summary()

# Определите оптимизатор и функцию потерь
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
sa_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("sa_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_sa_unet = sa_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=2,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
sa_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_sa_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("sa_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("sa_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_sa_unet.history, history_file)

# Save the final trained weights
sa_unet.save_weights("sa_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_sa_unet.history['loss'], label='Training Loss')
plt.plot(history_sa_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_sa_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_sa_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_sa_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_sa_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_sa_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_sa_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_sa_unet.history['precision'], label='Training Precision')
plt.plot(history_sa_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_sa_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_sa_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_sa_unet.history['recall'], label='Training Recall')
plt.plot(history_sa_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

test_metrics = sa_unet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(sa_unet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del sa_unet
gc.collect()

"""**RA_unet**"""

from tensorflow.keras import models, layers, regularizers
from tensorflow.keras import backend as K

# Слой для получения сигнала врат (gating signal)
def gatingsignal(input, out_size, batchnorm=False):
    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x

# Attention блок
def attention_block(x, gating, inter_shape):
    shape_x = K.int_shape(x)
    shape_g = K.int_shape(gating)
    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), kernel_initializer='he_normal', padding='same')(x)
    shape_theta_x = K.int_shape(theta_x)
    phi_g = layers.Conv2D(inter_shape, (1, 1), kernel_initializer='he_normal', padding='same')(gating)
    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3), strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]), kernel_initializer='he_normal', padding='same')(phi_g)
    concat_xg = layers.add([upsample_g, theta_x])
    act_xg = layers.Activation('relu')(concat_xg)
    psi = layers.Conv2D(1, (1, 1), kernel_initializer='he_normal', padding='same')(act_xg)
    sigmoid_xg = layers.Activation('sigmoid')(psi)
    shape_sigmoid = K.int_shape(sigmoid_xg)
    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)
    upsample_psi = layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': shape_x[3]})(upsample_psi)
    y = layers.multiply([upsample_psi, x])
    result = layers.Conv2D(shape_x[3], (1, 1), kernel_initializer='he_normal', padding='same')(y)
    attenblock = layers.BatchNormalization()(result)
    return attenblock

# Residual блок
def res_conv_block(x, kernelsize, filters, dropout, batchnorm=False):
    conv1 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(x)
    if batchnorm:
        conv1 = layers.BatchNormalization(axis=3)(conv1)
    conv1 = layers.Activation('relu')(conv1)
    conv2 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(conv1)
    if batchnorm:
        conv2 = layers.BatchNormalization(axis=3)(conv2)
        conv2 = layers.Activation("relu")(conv2)
    if dropout > 0:
        conv2 = layers.Dropout(dropout)(conv2)
    # Skip connection
    shortcut = layers.Conv2D(filters, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(x)
    if batchnorm:
        shortcut = layers.BatchNormalization(axis=3)(shortcut)
    shortcut = layers.Activation("relu")(shortcut)
    respath = layers.add([shortcut, conv2])
    return respath

# Residual-Attention U-Net (RA-UNET)
def residual_attentionunet(input_shape, dropout=0.2, batchnorm=True):
    filters = [16, 32, 64, 128, 256]
    kernelsize = 3
    upsample_size = 2

    inputs = layers.Input(input_shape)

    # Downsampling layers
    dn_1 = res_conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)
    pool1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)

    dn_2 = res_conv_block(pool1, kernelsize, filters[1], dropout, batchnorm)
    pool2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)

    dn_3 = res_conv_block(pool2, kernelsize, filters[2], dropout, batchnorm)
    pool3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)

    dn_4 = res_conv_block(pool3, kernelsize, filters[3], dropout, batchnorm)
    pool4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)

    dn_5 = res_conv_block(pool4, kernelsize, filters[4], dropout, batchnorm)

    # Upsampling layers
    gating_5 = gatingsignal(dn_5, filters[3], batchnorm)
    att_5 = attention_block(dn_4, gating_5, filters[3])
    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(dn_5)
    up_5 = layers.concatenate([up_5, att_5], axis=3)
    up_conv_5 = res_conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)

    gating_4 = gatingsignal(up_conv_5, filters[2], batchnorm)
    att_4 = attention_block(dn_3, gating_4, filters[2])
    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(up_conv_5)
    up_4 = layers.concatenate([up_4, att_4], axis=3)
    up_conv_4 = res_conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)

    gating_3 = gatingsignal(up_conv_4, filters[1], batchnorm)
    att_3 = attention_block(dn_2, gating_3, filters[1])
    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(up_conv_4)
    up_3 = layers.concatenate([up_3, att_3], axis=3)
    up_conv_3 = res_conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)

    gating_2 = gatingsignal(up_conv_3, filters[0], batchnorm)
    att_2 = attention_block(dn_1, gating_2, filters[0])
    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(up_conv_3)
    up_2 = layers.concatenate([up_2, att_2], axis=3)
    up_conv_2 = res_conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)

    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)
    conv_final = layers.BatchNormalization(axis=3)(conv_final)
    outputs = layers.Activation('sigmoid')(conv_final)

    model = models.Model(inputs=[inputs], outputs=[outputs])
    model.summary()
    return model

image_height = 256
image_width = 256
num_image_channels = 3
num_mask_channels = 1

ra_unet = residual_attentionunet(input_shape=(image_height, image_width, num_image_channels), dropout=0.2, batchnorm=True)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
ra_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

# Print the summary of the CE-Net model
ra_unet.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("ra_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_ra_unet = ra_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=2,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
ra_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_ra_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("ra_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("ra_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_ra_unet.history, history_file)

# Save the final trained weights
ra_unet.save_weights("ra_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_ra_unet.history['loss'], label='Training Loss')
plt.plot(history_ra_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_ra_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_ra_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_ra_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_ra_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_ra_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_ra_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_ra_unet.history['precision'], label='Training Precision')
plt.plot(history_ra_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_ra_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_ra_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_ra_unet.history['recall'], label='Training Recall')
plt.plot(history_ra_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

test_metrics = ra_unet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(ra_unet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del ra_unet
gc.collect()

"""**DanNucNet**"""

from tensorflow.keras.layers import (
    Input, Conv2D, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, AveragePooling2D,
    concatenate, add, multiply
)
from tensorflow.keras import Model, optimizers
from tensorflow.keras.initializers import Constant
from tensorflow.keras.activations import relu

def spa_atten(enc_feat, dec_feat):
    enc_line = AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding="same")(enc_feat)
    enc_line = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding="same")(enc_line)
    enc_line = Conv2D(filters=1, kernel_size=(1, 1), activation=None, padding='same',
                      kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(enc_line)
    enc_line = Conv2D(filters=1, kernel_size=(7, 7), activation=None, padding='same',
                      kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(enc_line)

    dec_line = AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding="same")(dec_feat)
    dec_line = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding="same")(dec_line)
    dec_line = Conv2D(filters=1, kernel_size=(1, 1), activation=None, padding='same',
                      kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(dec_line)
    dec_line = Conv2D(filters=1, kernel_size=(7, 7), activation=None, padding='same',
                      kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(dec_line)

    out = Activation('sigmoid')(add([enc_line, dec_line]))

    return out

def cha_atten(enc_feat, dec_feat, filters):
    enc_line_a = AveragePooling2D(pool_size=(2, 2), strides=(1, 1),  padding="same")(enc_feat)
    enc_line_a = Conv2D(filters=filters // 16,    kernel_size=(1, 1), activation=None,  padding='same',
                        kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(enc_line_a)
    enc_line_m = MaxPooling2D(pool_size=(2, 2),  strides=(1, 1),     padding="same")(enc_feat)
    enc_line_m = Conv2D(filters=filters // 16,    kernel_size=(1, 1), activation=None,  padding='same',
                        kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(enc_line_m)
    enc_line = add([enc_line_a, enc_line_m])
    enc_line = Conv2D(filters=filters // 16,    kernel_size=(1, 1), activation=None,  padding='same',
                      kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(enc_line)

    dec_line_a = AveragePooling2D(pool_size=(2, 2), strides=(1, 1),  padding="same")(dec_feat)
    dec_line_a = Conv2D(filters=filters // 16,    kernel_size=(1, 1), activation=None,  padding='same',
                        kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(dec_line_a)
    dec_line_m = MaxPooling2D(pool_size=(2, 2),  strides=(1, 1),     padding="same")(dec_feat)
    dec_line_m = Conv2D(filters=filters // 16,    kernel_size=(1, 1), activation=None,  padding='same',
                        kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(dec_line_m)
    dec_line = add([dec_line_a, dec_line_m])
    dec_line = Conv2D(filters=filters // 16,    kernel_size=(1, 1), activation=None,  padding='same',
                      kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(dec_line)

    out = add([enc_line, dec_line])
    out = Conv2D(filters=filters, kernel_size=(1, 1), activation='sigmoid', padding='same',
                 kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(out)

    return out

def spa_cha_atten_gate(enc_feat, dec_feat, filters):
    spa_aten_gate = spa_atten(enc_feat, dec_feat)
    cha_aten_gate = cha_atten(enc_feat, dec_feat, filters)

    mul = multiply([spa_aten_gate, enc_feat])
    mul = multiply([cha_aten_gate, mul])

    cat = concatenate([mul, dec_feat], axis=-1)

    out = Conv2D(filters=filters, kernel_size=(3, 3), activation='relu', padding='same',
                 kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(cat)
    out = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01), gamma_initializer=Constant(1.0),
                             momentum=0.5)(out)

    return out

def conv_block(input, filters):
    x = Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='glorot_normal',
               bias_initializer=Constant(0.1))(input)
    x = Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='glorot_normal',
               bias_initializer=Constant(0.1))(x)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0), gamma_initializer=Constant(1.0),
                           momentum=0.5)(x)
    return x

def bottleneck_block(input, filters):
    x = Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='glorot_normal',
               bias_initializer=Constant(0.1))(input)
    x = Conv2D(filters, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer='glorot_normal',
               bias_initializer=Constant(0.1))(x)
    x = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.0), gamma_initializer=Constant(1.0),
                           momentum=0.5)(x)
    return x

def robust_residual_block(input, filt):
    xa_0 = Conv2D(filters=filt, kernel_size=(1, 1), padding='same', activation='relu',
                  kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input)
    bn_0 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01), gamma_initializer=Constant(1.0),
                              momentum=0.5)(xa_0)

    xa_1 = Conv2D(filters=filt, kernel_size=(3, 3), padding='same', activation='relu',
                  kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(bn_0)
    bn_1 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01), gamma_initializer=Constant(1.0),
                              momentum=0.5)(xa_1)

    xa_2 = Conv2D(filters=filt, kernel_size=(1, 1), padding='same', activation='relu',
                  kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(bn_1)
    bn_2 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01), gamma_initializer=Constant(1.0),
                              momentum=0.5)(xa_2)

    xb_0 = Conv2D(filters=filt, kernel_size=(1, 1), padding='same', activation='relu',
                  kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(input)
    bn_3 = BatchNormalization(epsilon=1e-3, beta_initializer=Constant(0.01), gamma_initializer=Constant(1.0),
                              momentum=0.5)(xb_0)
    x = concatenate([bn_2, bn_3], axis=-1)
    x = Conv2D(filters=filt, kernel_size=(3, 3), activation='relu', padding='same', use_bias=True,
               kernel_initializer='glorot_normal', bias_initializer=Constant(0.1))(x)

    return x

input_shape = (256, 256, 3)
inputs = Input(input_shape)
filters = [32, 64, 128, 256, 512]

r_conv1 = robust_residual_block(inputs, filters[0])
pool = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(r_conv1)

r_conv2 = robust_residual_block(pool, filters[1])
pool = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(r_conv2)

r_conv3 = robust_residual_block(pool, filters[2])
pool = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(r_conv3)

r_conv4 = robust_residual_block(pool, filters[3])
pool = MaxPooling2D((2, 2), strides=(2, 2), padding="same")(r_conv4)

r_conv5 = bottleneck_block(pool, filters[4])

up_sam_1 = Conv2DTranspose(filters[3], (3, 3), strides=(2, 2), padding='same')(r_conv5)
up_conv1 = spa_cha_atten_gate(r_conv4, up_sam_1, filters[3])

up_sam_2 = Conv2DTranspose(filters[2], (3, 3), strides=(2, 2), padding='same')(up_conv1)
up_conv2 = spa_cha_atten_gate(r_conv3, up_sam_2, filters[2])

up_sam_3 = Conv2DTranspose(filters[1], (3, 3), strides=(2, 2), padding='same')(up_conv2)
up_conv3 = spa_cha_atten_gate(r_conv2, up_sam_3, filters[1])

up_sam_4 = Conv2DTranspose(filters[0], (3, 3), strides=(2, 2), padding='same')(up_conv3)
up_conv4 = spa_cha_atten_gate(r_conv1, up_sam_4, filters[0])

cat = Conv2D(filters[0], kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same')(up_conv4)

outputs = Conv2D(1, kernel_size=(1, 1), strides=(1, 1), activation='sigmoid', padding='same')(cat)

DanNucNet_model = Model(inputs=[inputs], outputs=[outputs])

DanNucNet_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

DanNucNet_model.summary()

import time
import pickle
import pandas as pd
from tensorflow.keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("DanNucNet_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_DanNucNet = DanNucNet_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_DanNucNet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("DanNucNet_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("DanNucNet_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_DanNucNet.history, history_file)

# Save the final trained weights
DanNucNet_model.save_weights("DanNucNet_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_DanNucNet.history['loss'], label='Training Loss')
plt.plot(history_DanNucNet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_DanNucNet.history['accuracy'], label='Training Accuracy')
plt.plot(history_DanNucNet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_DanNucNet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_DanNucNet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_DanNucNet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_DanNucNet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_DanNucNet.history['precision'], label='Training Precision')
plt.plot(history_DanNucNet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_DanNucNet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_DanNucNet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_DanNucNet.history['recall'], label='Training Recall')
plt.plot(history_DanNucNet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

test_metrics = DanNucNet_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(DanNucNet_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del DanNucNet_model
gc.collect()

"""**FF_UNet**"""

import cv2
import copy
import numpy as np
import tensorflow as tf
from keras import backend as K
import matplotlib.pyplot as plt
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.layers import add
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.layers import Dense



def expend_as(x, n):
    y = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': n})(x)
    return y

def conv_bn_act(x, filters, drop_out=0.0):
    x = Conv2D(filters, (3, 3), activation=None, padding='same')(x)

    if drop_out > 0:
        x = Dropout(drop_out)(x)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    return x

def attention_layer(d, e, n):
    d1 = Conv2D(n, (1, 1), activation=None, padding='same')(d)
    e1 = Conv2D(n, (1, 1), activation=None, padding='same')(e)

    concat_de = add([d1, e1])

    relu_de = Activation('relu')(concat_de)
    conv_de = Conv2D(1, (1, 1), padding='same')(relu_de)
    sigmoid_de = Activation('sigmoid')(conv_de)

    shape_e = K.int_shape(e)
    upsample_psi = expend_as(sigmoid_de, shape_e[3])

    return multiply([upsample_psi, e])

def feature_fused_module(x, filters, compression=0.5, drop_out=0.0):
    x1 = Conv2D(filters, (3, 3), dilation_rate=2, padding='same')(x)

    if drop_out > 0:
        x1 = Dropout(drop_out)(x1)

    x1 = BatchNormalization()(x1)
    x1 = Activation('relu')(x1)

    x2 = Conv2D(filters, (3, 3), padding='same')(x)

    if drop_out > 0:
        x2 = Dropout(drop_out)(x2)

    x2 = BatchNormalization()(x2)
    x2 = Activation('relu')(x2)

    x3 = add([x1, x2])

    x3 = GlobalAveragePooling2D()(x3)

    x3 = Dense(int(filters * compression))(x3)
    x3 = BatchNormalization()(x3)
    x3 = Activation('relu')(x3)

    x3 = Dense(filters)(x3)

    x3p = Activation('sigmoid')(x3)

    x3m = Lambda(lambda x: 1 - x)(x3p)

    x4 = multiply([x1, x3p])
    x5 = multiply([x2, x3m])

    return add([x4, x5])

def FF_UNet(input_shape=(256, 256, 3), filters=16, compression=0.5, drop_out=0, half_net=False, attention_gates=False):

    inputShape = Input(input_shape)

    c1 = feature_fused_module(inputShape, filters, compression=compression, drop_out=drop_out)
    c1 = feature_fused_module(c1, filters, compression=compression, drop_out=drop_out)
    p1 = MaxPooling2D((2, 2))(c1)
    filters = 2 * filters

    c2 = feature_fused_module(p1, filters, compression=compression, drop_out=drop_out)
    c2 = feature_fused_module(c2, filters, compression=compression, drop_out=drop_out)
    p2 = MaxPooling2D((2, 2))(c2)
    filters = 2 * filters

    c3 = feature_fused_module(p2, filters, compression=compression, drop_out=drop_out)
    c3 = feature_fused_module(c3, filters, compression=compression, drop_out=drop_out)
    p3 = MaxPooling2D((2, 2))(c3)
    filters = 2 * filters

    c4 = feature_fused_module(p3, filters, compression=compression, drop_out=drop_out)
    c4 = feature_fused_module(c4, filters, compression=compression, drop_out=drop_out)
    p4 = MaxPooling2D((2, 2))(c4)
    filters = 2 * filters

    cm = feature_fused_module(p4, filters, compression=compression, drop_out=drop_out)
    cm = feature_fused_module(cm, filters, compression=compression, drop_out=drop_out)

    filters = filters // 2

    u4 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(cm)

    if attention_gates:
        u4 = concatenate([u4, attention_layer(u4, c4, 1)], axis=3)
    else:
        u4 = concatenate([u4, c4], axis=3)

    if half_net:
        c5 = conv_bn_act(u4, filters, drop_out=drop_out)
        c5 = conv_bn_act(c5, filters, drop_out=drop_out)
    else:
        c5 = feature_fused_module(u4, filters, compression=compression, drop_out=drop_out)
        c5 = feature_fused_module(c5, filters, compression=compression, drop_out=drop_out)

    filters = filters // 2

    u3 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(c5)

    if attention_gates:
        u3 = concatenate([u3, attention_layer(u3, c3, 1)], axis=3)
    else:
        u3 = concatenate([u3, c3], axis=3)

    if half_net:
        c6 = conv_bn_act(u3, filters, drop_out=drop_out)
        c6 = conv_bn_act(c6, filters, drop_out=drop_out)
    else:
        c6 = feature_fused_module(u3, filters, compression=compression, drop_out=drop_out)
        c6 = feature_fused_module(c6, filters, compression=compression, drop_out=drop_out)

    filters = filters // 2

    u2 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(c6)

    if attention_gates:
        u2 = concatenate([u2, attention_layer(u2, c2, 1)], axis=3)
    else:
        u2 = concatenate([u2, c2], axis=3)

    if half_net:
        c7 = conv_bn_act(u2, filters, drop_out=drop_out)
        c7 = conv_bn_act(c7, filters, drop_out=drop_out)

    else:
        c7 = feature_fused_module(u2, filters, compression=compression, drop_out=drop_out)
        c7 = feature_fused_module(c7, filters, compression=compression, drop_out=drop_out)

    filters = filters // 2

    u1 = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='same')(c7)

    if attention_gates:
        u1 = concatenate([u1, attention_layer(u1, c1, 1)], axis=3)
    else:
        u1 = concatenate([u1, c1], axis=3)

    if half_net:
        c8 = conv_bn_act(u1, filters, drop_out=drop_out)
        c8 = conv_bn_act(c8, filters, drop_out=drop_out)
    else:
        c8 = feature_fused_module(u1, filters, compression=compression, drop_out=drop_out)
        c8 = feature_fused_module(c8, filters, compression=compression, drop_out=drop_out)

    c9 = Conv2D(1, (1, 1), padding="same", activation='sigmoid')(c8)

    return Model(inputs=[inputShape], outputs=[c9])

input_shape = (256, 256, 3)  # Размер входного изображения
num_classes = 1  # Один класс (бинарная сегментация)

# Создайте модель и присвойте ей имя в переменной FF_UNet_model
FF_UNet_model = FF_UNet(input_shape=input_shape, filters=16, compression=0.5, drop_out=0, attention_gates=False)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = tf.keras.losses.BinaryCrossentropy()

# Compile the UNet model with the desired optimizer and loss
FF_UNet_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])
# Выведите информацию о модели
FF_UNet_model.summary()

import time
import pickle
import pandas as pd
from tensorflow.keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("FF_UNet_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_FF_UNet_model= FF_UNet_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=2,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
FF_UNet_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_FF_UNet_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("FF_UNet_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("FF_UNet_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_FF_UNet_model.history, history_file)

# Save the final trained weights
FF_UNet_model.save_weights("FF_UNet_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_FF_UNet_model.history['loss'], label='Training Loss')
plt.plot(history_FF_UNet_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_FF_UNet_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_FF_UNet_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_FF_UNet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_FF_UNet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_FF_UNet_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_FF_UNet_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_FF_UNet_model.history['precision'], label='Training Precision')
plt.plot(history_FF_UNet_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_FF_UNet_model.history['recall'], label='Training Recall')
plt.plot(history_FF_UNet_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

test_metrics = FF_UNet_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(FF_UNet_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del FF_UNet_model
gc.collect()

"""**Attention_Unet**"""

from tensorflow.keras.layers import Layer, Conv2D, Dropout, MaxPool2D, UpSampling2D, concatenate, Add, Multiply, BatchNormalization, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.metrics import Accuracy

class EncoderBlock(Layer):
    def __init__(self, filters, rate, pooling=True, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.filters = filters
        self.rate = rate
        self.pooling = pooling

        self.c1 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')
        self.c2 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')
        self.drop = Dropout(rate)
        self.pool = MaxPool2D()

    def call(self, X):
        x = self.c1(X)
        x = self.c2(x)
        x = self.drop(x)
        if self.pooling:
            y = self.pool(x)
            return y, x
        else:
            return x

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "rate": self.rate,
            "pooling": self.pooling
        }

class DecoderBlock(Layer):
    def __init__(self, filters, rate, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.filters = filters
        self.rate = rate

        self.up = UpSampling2D()
        self.net = EncoderBlock(filters, rate, pooling=False)

    def call(self, X):
        X, skip_X = X
        x = self.up(X)
        c_ = concatenate([x, skip_X])
        x = self.net(c_)
        return x

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "rate": self.rate
        }

class AttentionGate(Layer):
    def __init__(self, filters, bn, **kwargs):
        super(AttentionGate, self).__init__(**kwargs)
        self.filters = filters
        self.bn = bn

        self.normal = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.down = Conv2D(filters, kernel_size=3, strides=2, padding='same', activation='relu', kernel_initializer='he_normal')
        self.learn = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')
        self.resample = UpSampling2D()
        self.BN = BatchNormalization()

    def call(self, X):
        X, skip_X = X

        x = self.normal(X)
        skip = self.down(skip_X)
        x = Add()([x, skip])
        x = self.learn(x)
        x = self.resample(x)
        f = Multiply()([x, skip_X])
        if self.bn:
            return self.BN(f)
        else:
            return f

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "bn": self.bn
        }

# Inputs
input_layer = Input(shape=(256, 256, 3))

# Encoder
p1, c1 = EncoderBlock(64, 0.1, name="Encoder1")(input_layer)
p2, c2 = EncoderBlock(128, 0.1, name="Encoder2")(p1)
p3, c3 = EncoderBlock(256, 0.2, name="Encoder3")(p2)
p4, c4 = EncoderBlock(512, 0.2, name="Encoder4")(p3)

# Encoding
encoding = EncoderBlock(512, 0.3, pooling=False, name="Encoding")(p4)

# Attention + Decoder

a1 = AttentionGate(256, bn=True, name="Attention1")([encoding, c4])
d1 = DecoderBlock(256, 0.2, name="Decoder1")([encoding, a1])

a2 = AttentionGate(128, bn=True, name="Attention2")([d1, c3])
d2 = DecoderBlock(128, 0.2, name="Decoder2")([d1, a2])

a3 = AttentionGate(64, bn=True, name="Attention3")([d2, c2])
d3 = DecoderBlock(64, 0.1, name="Decoder3")([d2, a3])

a4 = AttentionGate(32, bn=True, name="Attention4")([d3, c1])
d4 = DecoderBlock(32, 0.1, name="Decoder4")([d3, a4])

# Output
output_layer = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(d4)

# Model
AttentionUNet_model = Model(inputs=[input_layer], outputs=[output_layer])

# Define the optimizer and loss function
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = tf.keras.losses.BinaryCrossentropy()

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
AttentionUNet_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])


# Print the model summary
AttentionUNet_model.summary()

import time
import pickle
import pandas as pd
from tensorflow.keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("AttentionUNet_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_AttentionUNet_model= AttentionUNet_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
AttentionUNet_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_AttentionUNet_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("AttentionUNet_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("AttentionUNet_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_AttentionUNet_model.history, history_file)

# Save the final trained weights
AttentionUNet_model.save_weights("AttentionUNet_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = AttentionUNet_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_AttentionUNet_model.history['loss'], label='Training Loss')
plt.plot(history_AttentionUNet_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_AttentionUNet_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_AttentionUNet_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_AttentionUNet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_AttentionUNet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_AttentionUNet_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_AttentionUNet_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_AttentionUNet_model.history['precision'], label='Training Precision')
plt.plot(history_AttentionUNet_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_AttentionUNet_model.history['recall'], label='Training Recall')
plt.plot(history_AttentionUNet_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(AttentionUNet_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del AttentionUNet_model
gc.collect()

"""**Attention_UNet++**"""

from tensorflow.keras.layers import Layer, Conv2D, Dropout, MaxPool2D, UpSampling2D, concatenate, Add, Multiply, BatchNormalization, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Define Attention Gate block
class AttentionGate(Layer):
    def __init__(self, filters, bn=True, **kwargs):
        super(AttentionGate, self).__init__(**kwargs)
        self.filters = filters
        self.bn = bn

        self.normal = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        # Additional layers
        self.extra_conv1 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv2 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv3 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv4 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        # End of additional layers
        self.down = Conv2D(filters, kernel_size=3, strides=2, padding='same', activation='relu', kernel_initializer='he_normal')
        self.learn = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')
        self.resample = UpSampling2D()
        self.BN = BatchNormalization()

    def call(self, X):
        X, skip_X = X

        x = self.normal(X)
        # Additional layers
        x = self.extra_conv1(x)
        x = self.extra_conv2(x)
        x = self.extra_conv3(x)
        x = self.extra_conv4(x)
        # End of additional layers
        skip = self.down(skip_X)
        x = Add()([x, skip])
        x = self.learn(x)
        x = self.resample(x)
        f = Multiply()([x, skip_X])
        if self.bn:
            return self.BN(f)
        else:
            return f

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "bn": self.bn
        }

# Define Encoder block
class EncoderBlock(Layer):
    def __init__(self, filters, rate, pooling=True, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.filters = filters
        self.rate = rate
        self.pooling = pooling

        self.c1 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')
        self.c2 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')
        self.drop = Dropout(rate)
        self.pool = MaxPool2D()
        # Additional layers
        self.extra_conv1 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv2 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv3 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv4 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        # End of additional layers

    def call(self, X):
        x = self.c1(X)
        x = self.c2(x)
        x = self.drop(x)
        # Additional layers
        x = self.extra_conv1(x)
        x = self.extra_conv2(x)
        x = self.extra_conv3(x)
        x = self.extra_conv4(x)
        # End of additional layers
        if self.pooling:
            y = self.pool(x)
            return y, x
        else:
            return x

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "rate": self.rate,
            "pooling": self.pooling
        }

# Define Decoder block
class DecoderBlock(Layer):
    def __init__(self, filters, rate, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.filters = filters
        self.rate = rate

        self.up = UpSampling2D()
        self.net = EncoderBlock(filters, rate, pooling=False)
        # Additional layers
        self.extra_conv1 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv2 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv3 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.extra_conv4 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        # End of additional layers

    def call(self, X):
        X, skip_X = X
        x = self.up(X)
        c_ = concatenate([x, skip_X])
        x = self.net(c_)
        # Additional layers
        x = self.extra_conv1(x)
        x = self.extra_conv2(x)
        x = self.extra_conv3(x)
        x = self.extra_conv4(x)
        # End of additional layers
        return x

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "rate": self.rate
        }

# Define att_unetpp_model
def att_unetpp_model():
    # Inputs
    input_layer = Input(shape=(256, 256, 3))

    # Encoder
    p1, c1 = EncoderBlock(64, 0.1, name="Encoder1")(input_layer)
    p2, c2 = EncoderBlock(128, 0.1, name="Encoder2")(p1)
    p3, c3 = EncoderBlock(256, 0.2, name="Encoder3")(p2)
    p4, c4 = EncoderBlock(512, 0.2, name="Encoder4")(p3)

    # Encoding
    encoding = EncoderBlock(512, 0.3, pooling=False, name="Encoding")(p4)

    # Attention + Decoder
    a1 = AttentionGate(256, bn=True, name="Attention1")([encoding, c4])
    d1 = DecoderBlock(256, 0.2, name="Decoder1")([encoding, a1])

    a2 = AttentionGate(128, bn=True, name="Attention2")([d1, c3])
    d2 = DecoderBlock(128, 0.2, name="Decoder2")([d1, a2])

    a3 = AttentionGate(64, bn=True, name="Attention3")([d2, c2])
    d3 = DecoderBlock(64, 0.1, name="Decoder3")([d2, a3])

    a4 = AttentionGate(32, bn=True, name="Attention4")([d3, c1])
    d4 = DecoderBlock(32, 0.1, name="Decoder4")([d3, a4])

    # Output
    output_layer = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(d4)

    # Model
    att_unetpp_model = Model(inputs=[input_layer], outputs=[output_layer])

    # Compile the model
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
    att_unetpp_model.compile(optimizer=optimizer,
                             loss='binary_crossentropy',
                             metrics=['accuracy', 'dice_coef', 'f1_score', 'recall', 'precision', 'mean_iou', 'sensitivity'])

    return att_unetpp_model

# Print the model summary
att_unetpp_model = att_unetpp_model()
att_unetpp_model.summary()

# Define the optimizer and loss function
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = tf.keras.losses.BinaryCrossentropy()

# Use the actual functions for metrics
metrics = ['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity]

# Compile the model
att_unetpp_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("att_unetpp_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_att_unetpp_model = att_unetpp_model.fit(
    train_images,
    train_masks,
    validation_data=(val_images, val_masks),
    batch_size=8,
    epochs=160,
    verbose=1,
    callbacks=[checkpoint_callback, history_callback]
)

end_time = time.time()
att_unetpp_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_att_unetpp_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("att_unetpp_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("att_unetpp_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_att_unetpp_model.history, history_file)

# Save the final trained weights
att_unetpp_model.save_weights("att_unetpp_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = att_unetpp_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_att_unetpp_model.history['loss'], label='Training Loss')
plt.plot(history_att_unetpp_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_att_unetpp_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_att_unetpp_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_att_unetpp_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_att_unetpp_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_att_unetpp_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_att_unetpp_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_att_unetpp_model.history['precision'], label='Training Precision')
plt.plot(history_att_unetpp_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_att_unetpp_model.history['recall'], label='Training Recall')
plt.plot(history_att_unetpp_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        #
        plt.subplot(1, 4, 4)

        #
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(att_unetpp_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del att_unetpp_model
gc.collect()

"""**Transfer Res UNet**"""

!pip install keras_unet_collection
from keras_unet_collection import models
help(models.transunet_2d)

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPooling2D, UpSampling2D, Add, concatenate, Activation, BatchNormalization
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import BinaryAccuracy

# Определение оптимизатора
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

def res_block(inputs, filter_size):
    # First Conv2D layer
    cb1 = Conv2D(filter_size, (3, 3), padding='same', activation="relu", kernel_regularizer=l2(1e-4))(inputs)
    cb1 = BatchNormalization()(cb1)
    # Second Conv2D layer parallel to the first one
    cb2 = Conv2D(filter_size, (1, 1), padding='same', activation="relu", kernel_regularizer=l2(1e-4))(inputs)
    cb2 = BatchNormalization()(cb2)
    # Addition of cb1 and cb2
    add = Add()([cb1, cb2])

    return add

def res_path(inputs, filter_size, path_number):
    # Minimum one residual block for every res path
    skip_connection = res_block(inputs, filter_size)

    # Two serial residual blocks for res path 2
    if path_number == 2:
        skip_connection = res_block(skip_connection, filter_size)

    # Three serial residual blocks for res path 1
    elif path_number == 1:
        skip_connection = res_block(skip_connection, filter_size)
        skip_connection = res_block(skip_connection, filter_size)

    return skip_connection

def decoder_block(inputs, mid_channels, out_channels):
    conv_kwargs = dict(
        activation='relu',
        padding='same',
        kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-4),
        data_format='channels_last'
    )

    # Upsampling (linear) layer
    db = UpSampling2D(size=(2, 2))(inputs)
    # First conv2D layer
    db = Conv2D(mid_channels, 3, **conv_kwargs)(db)
    db = BatchNormalization()(db)
    # Second conv2D layer
    db = Conv2D(out_channels, 3, **conv_kwargs)(db)
    db = BatchNormalization()(db)

    return db

def TransResUNet(input_size=(256, 256, 3)):
    # Input
    inputs = Input(input_size)
    inp = inputs
    input_shape = input_size

    # Handling input channels
    if input_size[-1] < 3:
        inp = Conv2D(3, 1)(inputs)
        input_shape = (input_size[0], input_size[0], 3)
    else:
        inp = inputs
        input_shape = input_size

    encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)

    enc1 = encoder.get_layer(name='block1_conv1')(inp)
    enc1 = encoder.get_layer(name='block1_conv2')(enc1)
    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)
    enc2 = encoder.get_layer(name='block2_conv1')(enc2)
    enc2 = encoder.get_layer(name='block2_conv2')(enc2)
    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)
    enc3 = encoder.get_layer(name='block3_conv1')(enc3)
    enc3 = encoder.get_layer(name='block3_conv2')(enc3)
    enc3 = encoder.get_layer(name='block3_conv3')(enc3)

    center = MaxPooling2D(pool_size=(2, 2))(enc3)
    center = decoder_block(center, 512, 256)

    res_path3 = res_path(enc3, 128, 3)
    dec3 = concatenate([res_path3, center], axis=3)
    dec3 = decoder_block(dec3, 256, 64)

    res_path2 = res_path(enc2, 64, 2)
    dec2 = concatenate([res_path2, dec3], axis=3)
    dec2 = decoder_block(dec2, 128, 64)

    res_path1 = res_path(enc1, 32, 1)
    dec1 = concatenate([res_path1, dec2], axis=3)
    dec1 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(dec1)
    dec1 = ReLU()(dec1)

    out = Conv2D(1, 1)(dec1)
    out = Activation('sigmoid')(out)

    model = Model(inputs=[inputs], outputs=[out])

    return model
#
input_size = (256, 256, 3)
transres_unet = TransResUNet(input_size)
transres_unet.summary()

# Определите оптимизатор и функцию потерь
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
transres_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("transres_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_transres_unet = transres_unet.fit(
    train_images,
    train_masks,
    validation_data=(val_images, val_masks),
    batch_size=8,
    epochs=160,
    verbose=1,
    callbacks=[checkpoint_callback, history_callback]
)

end_time = time.time()
transres_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_transres_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("transres_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("transres_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_transres_unet.history, history_file)

# Save the final trained weights
transres_unet.save_weights("atransres_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = transres_unet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(transres_unet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_transres_unet.history['loss'], label='Training Loss')
plt.plot(history_transres_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_transres_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_transres_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_transres_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_transres_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_transres_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_transres_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_transres_unet.history['precision'], label='Training Precision')
plt.plot(history_transres_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_transres_unet.history['recall'], label='Training Recall')
plt.plot(history_transres_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

"""**Inception_UNet**"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import MaxPool2D, Concatenate

def inception_block(input, num_filters):
    # 1x1 convolution
    conv1x1 = Conv2D(num_filters, 1, padding="same", activation="relu")(input)

    # 3x3 convolution
    conv3x3 = Conv2D(num_filters, 3, padding="same", activation="relu")(input)

    # 5x5 convolution
    conv5x5 = Conv2D(num_filters, 5, padding="same", activation="relu")(input)

    # MaxPooling
    max_pool = MaxPooling2D((3, 3), strides=(1, 1), padding="same")(input)
    max_pool_conv = Conv2D(num_filters, 1, padding="same", activation="relu")(max_pool)

    # Concatenate all the branches
    inception_module = concatenate([conv1x1, conv3x3, conv5x5, max_pool_conv], axis=-1)

    return inception_module

def conv_block(input, num_filters):
    inception_output = inception_block(input, num_filters)
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(inception_output)
    x = BatchNormalization()(x)
    return x

# Encoder block: Conv block followed by maxpooling
def encoder_block(input, num_filters):
    x = conv_block(input, num_filters)
    x = conv_block(x, num_filters)  # Additional convolution layer
    p = MaxPooling2D((2, 2))(x)
    return x, p

# Decoder block: skip features get input from encoder for concatenation
def decoder_block(input, skip_features, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = concatenate([x, skip_features], axis=-1)
    x = conv_block(x, num_filters)
    x = conv_block(x, num_filters)  # Additional convolution layer
    return x

# Build Unet using the blocks
def build_unet(input_shape, n_classes):
    inputs = Input(input_shape)

    s1, p1 = encoder_block(inputs, 32)
    s2, p2 = encoder_block(p1, 64)
    s3, p3 = encoder_block(p2, 128)
    s4, p4 = encoder_block(p3, 256)

    b1 = conv_block(p4, 512)  # Bridge

    d1 = decoder_block(b1, s4, 256)
    d2 = decoder_block(d1, s3, 128)
    d3 = decoder_block(d2, s2, 64)
    d4 = decoder_block(d3, s1, 32)

    if n_classes == 1:  # Binary
        activation = 'sigmoid'
    else:
        activation = 'softmax'

    outputs = Conv2D(1, 1, padding="same", activation=activation)(d4)  # Change the activation based on n_classes

    unet = Model(inputs, outputs, name="Inception-UNet")
    return unet

IMG_HEIGHT = 256
IMG_WIDTH = 256
IMG_CHANNELS = 3

input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)
inception_unet = build_unet(input_shape, n_classes=1)
inception_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])
inception_unet.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("inception_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_inception_unet = inception_unet.fit(
    train_images,
    train_masks,
    validation_data=(val_images, val_masks),
    batch_size=8,
    epochs=160,
    verbose=1,
    callbacks=[checkpoint_callback, history_callback]
)

end_time = time.time()
inception_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_inception_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("tinception_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("inception_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_transres_unet.history, history_file)

# Save the final trained weights
inception_unet.save_weights("inception_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = inception_unet.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_inception_unet.history['loss'], label='Training Loss')
plt.plot(history_inception_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_inception_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_inception_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_inception_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_inception_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_inception_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_inception_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_inception_unet.history['precision'], label='Training Precision')
plt.plot(history_inception_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_inception_unet.history['recall'], label='Training Recall')
plt.plot(history_inception_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(inception_unet, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del inception_unet
gc.collect()

"""**unet_resnet_101**"""

from keras.models import Model
from keras.layers import Input, Activation, Conv2D
from keras.layers import MaxPooling2D, BatchNormalization
from keras.layers import UpSampling2D
from keras.layers import concatenate
from keras.layers import add


def conv3x3(x, out_filters, strides=(1, 1)):
    x = Conv2D(out_filters, 3, padding='same', strides=strides, use_bias=False, kernel_initializer='he_normal')(x)
    return x


def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same', use_activation=True):
    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, kernel_initializer='he_normal')(x)
    x = BatchNormalization(axis=3)(x)
    if use_activation:
        x = Activation('relu')(x)
        return x
    else:
        return x


def basic_Block(input, out_filters, strides=(1, 1), with_conv_shortcut=False):
    x = conv3x3(input, out_filters, strides)
    x = BatchNormalization(axis=3)(x)
    x = Activation('relu')(x)

    x = conv3x3(x, out_filters)
    x = BatchNormalization(axis=3)(x)

    if with_conv_shortcut:
        residual = Conv2D(out_filters, 1, strides=strides, use_bias=False, kernel_initializer='he_normal')(input)
        residual = BatchNormalization(axis=3)(residual)
        x = add([x, residual])
    else:
        x = add([x, input])

    x = Activation('relu')(x)
    return x


def bottleneck_Block(input, out_filters, strides=(1, 1), with_conv_shortcut=False):
    expansion = 4
    de_filters = int(out_filters / expansion)

    x = Conv2D(de_filters, 1, use_bias=False, kernel_initializer='he_normal')(input)
    x = BatchNormalization(axis=3)(x)
    x = Activation('relu')(x)

    x = Conv2D(de_filters, 3, strides=strides, padding='same', use_bias=False, kernel_initializer='he_normal')(x)
    x = BatchNormalization(axis=3)(x)
    x = Activation('relu')(x)

    x = Conv2D(out_filters, 1, use_bias=False, kernel_initializer='he_normal')(x)
    x = BatchNormalization(axis=3)(x)

    if with_conv_shortcut:
        residual = Conv2D(out_filters, 1, strides=strides, use_bias=False, kernel_initializer='he_normal')(input)
        residual = BatchNormalization(axis=3)(residual)
        x = add([x, residual])
    else:
        x = add([x, input])

    x = Activation('relu')(x)
    return x


def unet_resnet_101(height, width, channel, classes):
    input = Input(shape=(height, width, channel))

    conv1_1 = Conv2D(64, 7, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(input)
    conv1_1 = BatchNormalization(axis=3)(conv1_1)
    conv1_1 = Activation('relu')(conv1_1)
    conv1_2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv1_1)

    # conv2_x  1/4
    conv2_1 = bottleneck_Block(conv1_2, 256, strides=(1, 1), with_conv_shortcut=True)
    conv2_2 = bottleneck_Block(conv2_1, 256)
    conv2_3 = bottleneck_Block(conv2_2, 256)

    # conv3_x  1/8
    conv3_1 = bottleneck_Block(conv2_3, 512, strides=(2, 2), with_conv_shortcut=True)
    conv3_2 = bottleneck_Block(conv3_1, 512)
    conv3_3 = bottleneck_Block(conv3_2, 512)
    conv3_4 = bottleneck_Block(conv3_3, 512)

    # conv4_x  1/16
    conv4_1 = bottleneck_Block(conv3_4, 1024, strides=(2, 2), with_conv_shortcut=True)
    conv4_2 = bottleneck_Block(conv4_1, 1024)
    conv4_3 = bottleneck_Block(conv4_2, 1024)
    conv4_4 = bottleneck_Block(conv4_3, 1024)
    conv4_5 = bottleneck_Block(conv4_4, 1024)
    conv4_6 = bottleneck_Block(conv4_5, 1024)
    conv4_7 = bottleneck_Block(conv4_6, 1024)
    conv4_8 = bottleneck_Block(conv4_7, 1024)
    conv4_9 = bottleneck_Block(conv4_8, 1024)
    conv4_10 = bottleneck_Block(conv4_9, 1024)
    conv4_11 = bottleneck_Block(conv4_10, 1024)
    conv4_12 = bottleneck_Block(conv4_11, 1024)
    conv4_13 = bottleneck_Block(conv4_12, 1024)
    conv4_14 = bottleneck_Block(conv4_13, 1024)
    conv4_15 = bottleneck_Block(conv4_14, 1024)
    conv4_16 = bottleneck_Block(conv4_15, 1024)
    conv4_17 = bottleneck_Block(conv4_16, 1024)
    conv4_18 = bottleneck_Block(conv4_17, 1024)
    conv4_19 = bottleneck_Block(conv4_18, 1024)
    conv4_20 = bottleneck_Block(conv4_19, 1024)
    conv4_21 = bottleneck_Block(conv4_20, 1024)
    conv4_22 = bottleneck_Block(conv4_21, 1024)
    conv4_23 = bottleneck_Block(conv4_22, 1024)

    # conv5_x  1/32
    conv5_1 = bottleneck_Block(conv4_23, 2048, strides=(2, 2), with_conv_shortcut=True)
    conv5_2 = bottleneck_Block(conv5_1, 2048)
    conv5_3 = bottleneck_Block(conv5_2, 2048)

    up6 = Conv2d_BN(UpSampling2D(size=(2, 2))(conv5_3), 1024, 2)
    merge6 = concatenate([conv4_23, up6], axis=3)
    conv6 = Conv2d_BN(merge6, 1024, 3)
    conv6 = Conv2d_BN(conv6, 1024, 3)

    up7 = Conv2d_BN(UpSampling2D(size=(2, 2))(conv6), 512, 2)
    merge7 = concatenate([conv3_4, up7], axis=3)
    conv7 = Conv2d_BN(merge7, 512, 3)
    conv7 = Conv2d_BN(conv7, 512, 3)

    up8 = Conv2d_BN(UpSampling2D(size=(2, 2))(conv7), 256, 2)
    merge8 = concatenate([conv2_3, up8], axis=3)
    conv8 = Conv2d_BN(merge8, 256, 3)
    conv8 = Conv2d_BN(conv8, 256, 3)

    up9 = Conv2d_BN(UpSampling2D(size=(2, 2))(conv8), 64, 2)
    merge9 = concatenate([conv1_1, up9], axis=3)
    conv9 = Conv2d_BN(merge9, 64, 3)
    conv9 = Conv2d_BN(conv9, 64, 3)

    up10 = Conv2d_BN(UpSampling2D(size=(2, 2))(conv9), 64, 2)
    conv10 = Conv2d_BN(up10, 64, 3)
    conv10 = Conv2d_BN(conv10, 64, 3)

    conv11 = Conv2d_BN(conv10, classes, 1, use_activation=None)
    activation = Activation('sigmoid', name='Classification')(conv11)

    unet_resnet_101 = Model(inputs=input, outputs=activation)
    return unet_resnet_101


unet_resnet_101_model = unet_resnet_101(height=256, width=256, channel=3, classes=1)


IMG_HEIGHT = 256
IMG_WIDTH = 256
IMG_CHANNELS = 3

input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)


unet_resnet_101_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])
unet_resnet_101_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("unet_resnet_101_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_unet_resnet_101_model = unet_resnet_101_model.fit(
    train_images,
    train_masks,
    validation_data=(val_images, val_masks),
    batch_size=8,
    epochs=160,
    verbose=1,
    callbacks=[checkpoint_callback, history_callback]
)

end_time = time.time()
unet_resnet_101_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_unet_resnet_101_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("unet_resnet_101_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("unet_resnet_101_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_unet_resnet_101_model.history, history_file)

# Save the final trained weights
unet_resnet_101_model.save_weights("unet_resnet_101_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = unet_resnet_101_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_unet_resnet_101_model.history['loss'], label='Training Loss')
plt.plot(history_unet_resnet_101_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_unet_resnet_101_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_unet_resnet_101_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unet_resnet_101_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unet_resnet_101_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_unet_resnet_101_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_unet_resnet_101_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_unet_resnet_101_model.history['precision'], label='Training Precision')
plt.plot(history_unet_resnet_101_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_unet_resnet_101_model.history['recall'], label='Training Recall')
plt.plot(history_unet_resnet_101_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(unet_resnet_101_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del unet_resnet_101_model
gc.collect()

"""***UNet_VGG16***"""

from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.applications import VGG16
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy

def unet_vgg16(input_shape=(256, 256, 3), num_classes=1):
    # Load pre-trained VGG16 model
    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

    # Use the layers from VGG16 as the contracting path
    contracting_layers = vgg16.get_layer('block5_conv3').output

    # Expansive Path
    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
        UpSampling2D(size=(2, 2))(contracting_layers))
    merge6 = concatenate([vgg16.get_layer('block4_conv3').output, up6], axis=3)
    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)

    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
        UpSampling2D(size=(2, 2))(conv6))
    merge7 = concatenate([vgg16.get_layer('block3_conv3').output, up7], axis=3)
    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)

    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
        UpSampling2D(size=(2, 2))(conv7))
    merge8 = concatenate([vgg16.get_layer('block2_conv2').output, up8], axis=3)
    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)

    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
        UpSampling2D(size=(2, 2))(conv8))
    merge9 = concatenate([vgg16.get_layer('block1_conv2').output, up9], axis=3)
    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)

    # Output
    outputs = Conv2D(num_classes, 1, activation='sigmoid')(conv9)

    # Create and compile the model
    model = Model(inputs=vgg16.input, outputs=outputs, name='unet_vgg16')

    return model

# Create the model
unet_vgg16_model = unet_vgg16(input_shape=(256, 256, 3), num_classes=1)

# Compile the model
optimizer = Adam(learning_rate=1e-4)
loss_function = BinaryCrossentropy()
unet_vgg16_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

# Print model summary
unet_vgg16_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("unet_vgg16_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_unet_vgg16_model = unet_vgg16_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
unet_vgg16_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_unet_vgg16_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("unet_vgg16_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("unet_vgg16_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_unet_vgg16_model.history, history_file)

# Save the final trained weights
unet_vgg16_model.save_weights("unet_vgg16_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = unet_vgg16_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_unet_vgg16_model.history['loss'], label='Training Loss')
plt.plot(history_unet_vgg16_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_unet_vgg16_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_unet_vgg16_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unet_vgg16_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unet_vgg16_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_unet_vgg16_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_unet_vgg16_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_unet_vgg16_model.history['precision'], label='Training Precision')
plt.plot(history_unet_vgg16_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_unet_vgg16_model.history['recall'], label='Training Recall')
plt.plot(history_unet_vgg16_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(unet_vgg16_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)

del unet_vgg16_model
gc.collect()

"""**Res_UNet**"""

import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda
# from tf.keras.optimizers import Adam
from keras.layers import Activation, MaxPool2D, Concatenate


def conv_block(input, num_filters):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)   #Not in the original network.
    x = Activation("relu")(x)

    x1 = Conv2D(num_filters, 3, padding="same")(x)
    x1 = BatchNormalization()(x1)  #Not in the original network
    x1 = Activation("relu")(x1)
    x2 =  x + x1

    x = Conv2D(num_filters, 3, padding="same")(x2)
    return x

#Encoder block: Conv block followed by maxpooling


def encoder_block(input, num_filters):
    x = conv_block(input, num_filters)
    p = MaxPool2D((2, 2))(x)
    return x, p

#Decoder block
#skip features gets input from encoder for concatenation

def decoder_block(input, skip_features, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, num_filters)
    return x

#Build Unet using the blocks
def build_resunet(input_shape, n_classes):
    inputs = Input(input_shape)

    s1, p1 = encoder_block(inputs, 32)
    s2, p2 = encoder_block(p1, 64)
    s3, p3 = encoder_block(p2, 128)
    s4, p4 = encoder_block(p3, 256)

    b1 = conv_block(p4, 512) #Bridge

    d1 = decoder_block(b1, s4, 256)
    d2 = decoder_block(d1, s3, 128)
    d3 = decoder_block(d2, s2, 64)
    d4 = decoder_block(d3, s1, 32)

    if n_classes == 1:  #Binary
      activation = 'sigmoid'
    else:
      activation = 'softmax'

    outputs = Conv2D(1, 1, padding="same", activation=activation)(d4)  #Change the activation based on n_classes
    print(activation)


    model = Model(inputs, outputs, name="Res-UNet")

    return model
from keras.optimizers import Adam

# Define input shape and number of classes
input_shape = (256, 256, 3)
num_classes = 1  # Assuming it's binary segmentation, change it for multi-class

# Build the Res-UNet model
resunet_model = build_resunet(input_shape, num_classes)


optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
resunet_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])


# Print the model summary
resunet_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("resunet_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_resunet_model= resunet_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=160,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
resunet_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_resunet_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("resunet_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("resunet_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_resunet_model.history, history_file)

# Save the final trained weights
resunet_model.save_weights("resunet_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

test_metrics = resunet_model.evaluate(test_images, test_masks, verbose=1)

# Extract and print the test metrics
test_loss = test_metrics[0]
test_accuracy = test_metrics[1]
test_dice_coef = test_metrics[2]
test_f1_score = test_metrics[3]
test_recall = test_metrics[4]
test_precision = test_metrics[5]
test_mean_iou = test_metrics[6]
test_sensitivity = test_metrics[7]

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)
print("Test Dice Coef:", test_dice_coef)
print("Test F1 Score:", test_f1_score)
print("Test Recall:", test_recall)
print("Test Precision:", test_precision)
print("Test Mean IOU:", test_mean_iou)
print("Test Sensitivity:", test_sensitivity)

import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_resunet_model.history['loss'], label='Training Loss')
plt.plot(history_resunet_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_resunet_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_resunet_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_resunet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_resunet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_resunet_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_resunet_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_resunet_model.history['precision'], label='Training Precision')
plt.plot(history_resunet_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_resunet_model.history['recall'], label='Training Recall')
plt.plot(history_resunet_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.colors as mcolors
from skimage import measure

prediction_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])

# Функция для визуализации снимков, масок и прогнозов с тонким контуром маски
def visualize_predictions(model, images, masks, prediction_cmap=None, num_samples=5, threshold=0.5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Преобразуем предсказание в бинарную маску с использованием порога
        prediction_binary = (prediction > threshold).astype(np.uint8)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))

        # Снимок
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Маска (черно-белая)
        plt.subplot(1, 4, 2)
        plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Предсказание с цветовыми метками
        plt.subplot(1, 4, 3)
        if prediction_cmap:
            plt.imshow(prediction.squeeze(), cmap=prediction_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='green')
        plt.title("Prediction")
        plt.axis('off')

        # Тонкий контур маски
        plt.subplot(1, 4, 4)

        # Используем библиотеку skimage для нахождения контура предсказания
        contours = measure.find_contours(prediction_binary.squeeze(), 0.5)

        for contour in contours:
            plt.plot(contour[:, 1], contour[:, 0], linewidth=1, color='yellow')

        plt.imshow(image, cmap='gray')
        plt.title("Thin Contour on Image")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(resunet_model, test_images[:30], test_masks[:30], prediction_cmap, num_samples=30, threshold=0.5)









from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/MyDrive/4-MonuseG/history_pkl/AttentionUNet_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/DanNucNet_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/FF_UNet_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/att_unetpp_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/base_unet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/deep_unet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/inception_unet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/nuclei_segnet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/ra_unet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/resunet_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/sa_unet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/transres_unet_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/unet_resnet_101_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/unet_vgg16_model_training_history.pkl /content
!cp /content/drive/MyDrive/4-MonuseG/history_pkl/unetcbam_model_training_history.pkl /content

import pickle
from tabulate import tabulate

file_paths = [
    "/content/unetcbam_model_training_history.pkl",
    "/content/unet_vgg16_model_training_history.pkl",
    "/content/unet_resnet_101_model_training_history.pkl",
    "/content/transres_unet_training_history.pkl",
    "/content/sa_unet_training_history.pkl",
    "/content/resunet_model_training_history.pkl",
    "/content/ra_unet_training_history.pkl",
    "/content/nuclei_segnet_training_history.pkl",
    "/content/inception_unet_training_history.pkl",
    "/content/deep_unet_training_history.pkl",
    "/content/base_unet_training_history.pkl",
    "/content/att_unetpp_model_training_history.pkl",
    "/content/FF_UNet_model_training_history.pkl",
    "/content/DanNucNet_model_training_history.pkl",
    "/content/AttentionUNet_model_training_history.pkl"
]

model_names = [
    "unetcbam_model",
    "unet_vgg16_model",
    "unet_resnet_101_model",
    "transres_unet",
    "sa_unet",
    "resunet_model",
    "ra_unet",
    "nuclei_segnet",
    "inception_unet",
    "deep_unet",
    "base_unet",
    "att_unetpp_model",
    "FF_UNet_model",
    "DanNucNet_model",
    "AttentionUNet_model"
]

models_data = []

for file_path in file_paths:
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
        models_data.append(history)

results = []

for i, model_data in enumerate(models_data):
    model_name = model_names[i]
    data = [
        model_name,
        model_data['loss'][-1],
        model_data['accuracy'][-1],
        model_data['dice_coef'][-1],
        model_data['f1_score'][-1],
        model_data['recall'][-1],
        model_data['precision'][-1],
        model_data['mean_iou'][-1],
        model_data['sensitivity'][-1]
    ]
    results.append(data)

headers = [
    "Model Name",
    "Loss",
    "Accuracy",
    "Dice Coef",
    "F1 Score",
    "Recall",
    "Precision",
    "Mean IOU",
    "Sensitivity"
]

table = tabulate(results, headers, tablefmt="pretty")
print(table)

import pickle
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
#
file_paths = [
    "/content/AttentionUNet_model_training_history.pkl",
    "/content/DanNucNet_model_training_history.pkl",
    "/content/FF_UNet_model_training_history.pkl",
    "/content/att_unetpp_model_training_history.pkl",
    "/content/base_unet_training_history.pkl",
    "/content/inception_unet_training_history.pkl",
    "/content/nuclei_segnet_training_history.pkl",
    "/content/ra_unet_training_history.pkl",
    "/content/resunet_model_training_history.pkl",
    "/content/sa_unet_training_history.pkl",
    "/content/transres_unet_training_history.pkl",
    "/content/unet_resnet_101_model_training_history.pkl",
    "/content/unet_vgg16_model_training_history.pkl",
    "/content/unetcbam_model_training_history.pkl",
    "/content/deep_unet_training_history.pkl"
]

model_names = [
    "Att-UNet",
    "DanNucNet",
    "FF_UNet",
    "Att_unet++",
    "Base_UNet",
    "Inception-UNet",
    "NucleiSegnet",
    "RA-UNet",
    "Res-UNet",
    "SA_UNet",
    "Trans-Res_UNet",
    "UNet_resnet_101",
    "UNet-VGG16",
    "UNet-CBAM",
    "Deep-UNet"
]

def load_training_history(file_path):
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
    return history

# Загрузка истории обучения для каждой модели
histories = [load_training_history(file_path) for file_path in file_paths]

# Определяем максимальное количество эпох среди всех моделей
max_epochs = max(len(history['dice_coef']) for history in histories)

# Создаем график
plt.figure(figsize=(8, 6))

colors = ['#46637F', '#C0B520', '#21419A', '#5DB9A9', '#D47D4E', '#303030', '#960B32', '#B8B8B8', '#FFA500', '#FF69B4', '#82FFFF', '#FFFF00', '#2F4F4F', '#00FF00', '#8A2BE2']
# Настройка стилей Seaborn с новой палитрой
sns.set(style="whitegrid", palette=sns.color_palette(colors))

for i, history in enumerate(histories):
    # Используем сглаживание с помощью numpy.convolve для улучшения визуализации
    smooth_dice_coef = np.convolve(history['dice_coef'], np.ones(10)/10, mode='valid')
    epochs = np.arange(1, len(smooth_dice_coef) + 1)

    # Заполняем NaN после завершения обучения
    extended_epochs = np.arange(1, max_epochs + 1)
    extended_smooth_dice_coef = np.full(max_epochs, np.nan)
    extended_smooth_dice_coef[:len(smooth_dice_coef)] = smooth_dice_coef

    plt.plot(extended_epochs, extended_smooth_dice_coef, 'o-', label=model_names[i], markersize=3, markevery=5, color=colors[i])


plt.xlabel('Epoch')
plt.ylabel('Dice Score')
plt.legend(ncol=3)

# Изменение цвета и стиля сетки
plt.grid(color='gray', linestyle='--', linewidth=0.5)

# Установка нужных меток по оси X
plt.xticks(np.arange(0, max_epochs + 1, 20))

plt.tight_layout()

# Отображение графика
plt.show()

import pickle
import matplotlib.pyplot as plt
import numpy as np

file_paths = [
    "/content/AttentionUNet_model_training_history.pkl",
    "/content/DanNucNet_model_training_history.pkl",
    "/content/FF_UNet_model_training_history.pkl",
    "/content/att_unetpp_model_training_history.pkl",
    "/content/base_unet_training_history.pkl",
    "/content/inception_unet_training_history.pkl",
    "/content/nuclei_segnet_training_history.pkl",
    "/content/ra_unet_training_history.pkl",
    "/content/resunet_model_training_history.pkl",
    "/content/sa_unet_training_history.pkl",
    "/content/transres_unet_training_history.pkl",
    "/content/unet_resnet_101_model_training_history.pkl",
    "/content/unet_vgg16_model_training_history.pkl",
    "/content/unetcbam_model_training_history.pkl",
    "/content/deep_unet_training_history.pkl"
]

model_names = [
    "Att-UNet",
    "DanNucNet",
    "FF_UNet",
    "Att_unet++",
    "Base_UNet",
    "Inception-UNet",
    "NucleiSegnet",
    "RA-UNet",
    "Res-UNet",
    "SA_UNet",
    "Trans-Res_UNet",
    "UNet_resnet_101",
    "UNet-VGG16",
    "UNet-CBAM",
    "Deep-UNet"
]

def load_training_history(file_path):
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
    return history

# Загрузка истории обучения для каждой модели
histories = [load_training_history(file_path) for file_path in file_paths]

# Определяем максимальное количество эпох среди всех моделей
max_epochs = max(len(history['loss']) for history in histories)

#
plt.figure(figsize=(8, 6))

colors = ['#46637F', '#C0B520', '#21419A', '#5DB9A9', '#D47D4E', '#303030', '#960B32', '#B8B8B8', '#FFA500', '#FF69B4', '#82FFFF', '#FFFF00', '#2F4F4F', '#00FF00', '#8A2BE2']

sns.set(style="whitegrid", palette=sns.color_palette(colors))

for i, history in enumerate(histories):
    #
    smooth_loss = np.convolve(history['loss'], np.ones(10)/10, mode='valid')
    epochs = np.arange(1, len(smooth_loss) + 1)

    #
    extended_epochs = np.arange(1, max_epochs + 1)
    extended_smooth_loss = np.full(max_epochs, np.nan)
    extended_smooth_loss[:len(smooth_loss)] = smooth_loss

    plt.plot(extended_epochs, extended_smooth_loss, 'o-', label=model_names[i], markersize=3, markevery=5, color=colors[i])

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(ncol=3)

# Изменение цвета и стиля сетки
plt.grid(color='gray', linestyle='--', linewidth=0.5)

# Установка нужных меток по оси X
plt.xticks(np.arange(0, max_epochs + 1, 20))

plt.tight_layout()

# Отображение графика
plt.show()

import pickle
import matplotlib.pyplot as plt
import numpy as np

# Список путей к файлам и имена моделей
file_paths = [
    "/content/AttentionUNet_model_training_history.pkl",
    "/content/DanNucNet_model_training_history.pkl",
    "/content/FF_UNet_model_training_history.pkl",
    "/content/att_unetpp_model_training_history.pkl",
    "/content/base_unet_training_history.pkl",
    "/content/inception_unet_training_history.pkl",
    "/content/nuclei_segnet_training_history.pkl",
    "/content/ra_unet_training_history.pkl",
    "/content/resunet_model_training_history.pkl",
    "/content/sa_unet_training_history.pkl",
    "/content/transres_unet_training_history.pkl",
    "/content/unet_resnet_101_model_training_history.pkl",
    "/content/unet_vgg16_model_training_history.pkl",
    "/content/unetcbam_model_training_history.pkl",
    "/content/deep_unet_training_history.pkl"
]

model_names = [
    "Att-UNet",
    "DanNucNet",
    "FF_UNet",
    "Att_unet++",
    "Base_UNet",
    "Inception-UNet",
    "NucleiSegnet",
    "RA-UNet",
    "Res-UNet",
    "SA_UNet",
    "Trans-Res_UNet",
    "UNet_resnet_101",
    "UNet-VGG16",
    "UNet-CBAM",
    "Deep-UNet"
]

def load_training_history(file_path):
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
    return history

# Загрузка истории обучения для каждой модели
histories = [load_training_history(file_path) for file_path in file_paths]

# Определяем максимальное количество эпох среди всех моделей
max_epochs = max(len(history['mean_iou']) for history in histories)

# Создаем график
plt.figure(figsize=(8, 6))

colors = ['#46637F', '#C0B520', '#21419A', '#5DB9A9', '#D47D4E', '#303030', '#960B32', '#B8B8B8', '#FFA500', '#FF69B4', '#82FFFF', '#FFFF00', '#2F4F4F', '#00FF00', '#8A2BE2']

for i, history in enumerate(histories):
    # Используем сглаживание с помощью numpy.convolve для улучшения визуализации
    smooth_mean_iou = np.convolve(history['mean_iou'], np.ones(10)/10, mode='valid')
    epochs = np.arange(1, len(smooth_mean_iou) + 1)

    # Заполняем NaN после завершения обучения
    extended_epochs = np.arange(1, max_epochs + 1)
    extended_smooth_mean_iou = np.full(max_epochs, np.nan)
    extended_smooth_mean_iou[:len(smooth_mean_iou)] = smooth_mean_iou

    plt.plot(extended_epochs, extended_smooth_mean_iou, 'o-', label=model_names[i], markersize=3, markevery=5, color=colors[i])

# Оформление графика
plt.title('Training Mean IOU for Different Models')
plt.xlabel('Epoch')
plt.ylabel('Mean IOU')
plt.legend(ncol=3)
plt.grid(True)

plt.grid(color='gray', linestyle='--', linewidth=0.5)

# Установка нужных меток по оси X
plt.xticks(np.arange(0, max_epochs + 1, 20))

plt.tight_layout()

# Отображение графика
plt.show()

import pickle
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Список путей к файлам и имена моделей
file_paths = [
    "/content/AttentionUNet_model_training_history.pkl",
    "/content/DanNucNet_model_training_history.pkl",
    "/content/FF_UNet_model_training_history.pkl",
    "/content/att_unetpp_model_training_history.pkl",
    "/content/base_unet_training_history.pkl",
    "/content/inception_unet_training_history.pkl",
    "/content/nuclei_segnet_training_history.pkl",
    "/content/ra_unet_training_history.pkl",
    "/content/resunet_model_training_history.pkl",
    "/content/sa_unet_training_history.pkl",
    "/content/transres_unet_training_history.pkl",
    "/content/unet_resnet_101_model_training_history.pkl",
    "/content/unet_vgg16_model_training_history.pkl",
    "/content/unetcbam_model_training_history.pkl",
    "/content/deep_unet_training_history.pkl"
]

model_names = [
    "Att-UNet",
    "DanNucNet",
    "FF_UNet",
    "Att_unet++",
    "Base_UNet",
    "Inception-UNet",
    "NucleiSegnet",
    "RA-UNet",
    "Res-UNet",
    "SA_UNet",
    "Trans-Res_UNet",
    "UNet_resnet_101",
    "UNet-VGG16",
    "UNet-CBAM",
    "Deep-UNet"
]

def load_training_history(file_path):
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
    return history

# Загрузка истории обучения для каждой модели
histories = [load_training_history(file_path) for file_path in file_paths]

# Определяем максимальное количество эпох среди всех моделей
max_epochs = max(len(history['f1_score']) for history in histories)

# Создаем график
plt.figure(figsize=(8, 6))


colors = ['#46637F', '#C0B520', '#21419A', '#5DB9A9', '#D47D4E', '#303030', '#960B32', '#B8B8B8', '#FFA500', '#FF69B4', '#82FFFF', '#FFFF00', '#2F4F4F', '#00FF00', '#8A2BE2']

sns.set(style="whitegrid", palette=sns.color_palette(colors))

for i, history in enumerate(histories):
    # Используем сглаживание с помощью numpy.convolve для улучшения визуализации
    smooth_f1_score = np.convolve(history['f1_score'], np.ones(10)/10, mode='valid')
    epochs = np.arange(1, len(smooth_f1_score) + 1)

    # Заполняем NaN после завершения обучения
    extended_epochs = np.arange(1, max_epochs + 1)
    extended_smooth_f1_score = np.full(max_epochs, np.nan)
    extended_smooth_f1_score[:len(smooth_f1_score)] = smooth_f1_score

    plt.plot(extended_epochs, extended_smooth_f1_score, 'o-', label=model_names[i], markersize=3, markevery=5, color=colors[i])


plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.legend(ncol=3)
plt.grid(True)
plt.grid(color='gray', linestyle='--', linewidth=0.5)
# Установка нужных меток по оси X
plt.xticks(np.arange(0, max_epochs + 1, 20))

plt.tight_layout()

# Отображение графика
plt.show()

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Specify the path to the folder with images
folder_path = '/content/data/processed-original-monuseg/train_folder/img'

# List to store color characteristics of the images
colors = []

# Iterate through each image file in the folder
for filename in os.listdir(folder_path):
    if filename.endswith(".png"):  # Assuming images are in PNG format
        image_path = os.path.join(folder_path, filename)
        image = cv2.imread(image_path)

        # Extract color information and add it to the list
        colors.extend(image.reshape(-1, 3))

# Convert the list of colors into a NumPy array
colors = np.array(colors)

# Set a stylish seaborn style
sns.set(style='whitegrid', palette='husl')

# Plot a combined histogram for color distributions with vertical lines filled
plt.figure(figsize=(10, 6))

plt.hist(colors[:, 0], bins=256, color='salmon', alpha=0.7, histtype='bar', label='Red', orientation='vertical')
plt.hist(colors[:, 1], bins=256, color='limegreen', alpha=0.7, histtype='bar', label='Green', orientation='vertical')
plt.hist(colors[:, 2], bins=256, color='royalblue', alpha=0.7, histtype='bar', label='Blue', orientation='vertical')


# Add grid for a more stylish look
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Set labels and legend
plt.xlabel('intensity', fontsize=14)
plt.ylabel('frequency', fontsize=14)

plt.legend()

plt.show()

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Specify the path to the folder with images
folder_path = '/content/resized_data/images'

# List to store color characteristics of the images
colors = []

# Iterate through each image file in the folder
for filename in os.listdir(folder_path):
    if filename.endswith(".png"):  # Assuming images are in PNG format
        image_path = os.path.join(folder_path, filename)
        image = cv2.imread(image_path)

        # Extract color information and add it to the list
        colors.extend(image.reshape(-1, 3))

# Convert the list of colors into a NumPy array
colors = np.array(colors)

# Set a stylish seaborn style
sns.set(style='whitegrid', palette='husl')

# Plot a combined histogram for color distributions with vertical lines filled
plt.figure(figsize=(10, 6))

plt.hist(colors[:, 0], bins=256, color='salmon', alpha=0.7, histtype='bar', label='Red', orientation='vertical')
plt.hist(colors[:, 1], bins=256, color='limegreen', alpha=0.7, histtype='bar', label='Green', orientation='vertical')
plt.hist(colors[:, 2], bins=256, color='royalblue', alpha=0.7, histtype='bar', label='Blue', orientation='vertical')


# Add grid for a more stylish look
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Set labels and legend
plt.xlabel('intensity', fontsize=14)
plt.ylabel('frequency', fontsize=14)

plt.legend()

plt.show()





















from google.colab import drive

# Подключитесь к Google Drive
drive.mount('/content/drive')

# Скопируйте файлы из Google Drive в Colab
!cp /content/drive/MyDrive/!!МонусеГ!!/unet_model_training_history.pkl /content/
!cp /content/drive/MyDrive/!!МонусеГ!!/unetcbam_model_training_history.pkl /content/
!cp /content/drive/MyDrive/!!МонусеГ!!/sa_unet_training_history.pkl /content/
!cp /content/drive/MyDrive/!!МонусеГ!!/ra_unet_training_history.pkl /content/
!cp /content/drive/MyDrive/!!МонусеГ!!/ce_net_training_history.pkl /content/
!cp /content/drive/MyDrive/!!МонусеГ!!/nuclei_segnet_training_history.pkl /content/

import pickle
from tabulate import tabulate

# Загрузите данные истории обучения из файлов
file_paths = [
    '/content/unet_model_training_history.pkl',
    '/content/sa_unet_training_history.pkl',
    '/content/ra_unet_training_history.pkl',
    '/content/nuclei_segnet_training_history.pkl',
    '/content/ce_net_training_history.pkl',
    '/content/unetcbam_model_training_history.pkl',
]

model_names = ['deep_unet', 'sa_unet', 'ra_unet', 'nuclei_segnet', 'ce_net', 'unetcbam']

models_data = []

for file_path in file_paths:
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
        models_data.append(history)

# Создайте список с результатами
results = []

for i, model_data in enumerate(models_data):
    model_name = model_names[i]
    data = [
        model_name,
        model_data['loss'][-1],
        model_data['accuracy'][-1],
        model_data['dice_coef'][-1],
        model_data['f1_score'][-1],
        model_data['recall'][-1],
        model_data['precision'][-1],
        model_data['mean_iou'][-1],
        model_data['sensitivity'][-1]
    ]
    results.append(data)

# Заголовки для столбцов
headers = [
    "Model Name",
    "Loss",
    "Accuracy",
    "Dice Coef",
    "F1 Score",
    "Recall",
    "Precision",
    "Mean IOU",
    "Sensitivity"
]

# Выведите красивую таблицу
table = tabulate(results, headers, tablefmt="pretty")
print(table)

import pickle
import matplotlib.pyplot as plt

file_paths = [
    '/content/unet_model_training_history.pkl',
    '/content/sa_unet_training_history.pkl',
    '/content/ra_unet_training_history.pkl',
    '/content/nuclei_segnet_training_history.pkl',
    '/content/ce_net_training_history.pkl',
    '/content/unetcbam_model_training_history.pkl',
]

model_names = ['deep_unet', 'sa_unet', 'ra_unet', 'nuclei_segnet', 'ce_net', 'unetcbam']

losses = []  # Список для хранения общего лосса
best_loss_values = []  # Список для хранения лучших значений потерь
best_loss_epochs = []  # Список для хранения номеров эпох с лучшими потерями

for file_path in file_paths:
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
        loss = history['loss']  # Используем метрику 'loss'
        min_loss_value = min(loss)  # Находим минимальное значение потерь
        best_loss_epoch = loss.index(min_loss_value)  # Находим номер эпохи с лучшей оценкой
        best_loss_values.append(min_loss_value)
        best_loss_epochs.append(best_loss_epoch)
        losses.append(loss)

plt.figure(figsize=(8, 6))
plt.style.use('seaborn-whitegrid')

for i, loss in enumerate(losses):
    plt.plot(loss, label=f'{model_names[i]} (Best: {best_loss_values[i]:.2f})', linewidth=2)
    plt.scatter(best_loss_epochs[i], best_loss_values[i], color='red', marker='o')

plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)

# Настройка цветов и стиля линий
colors = ['b', 'g', 'r', 'c', 'm', 'y']
for i, line in enumerate(plt.gca().get_lines()):
    line.set_color(colors[i])

# Настройка шрифта и цветов текста
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = 'Times New Roman'
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['axes.labelcolor'] = 'black'

# Отобразите график
plt.tight_layout()
plt.show()

import pickle
import matplotlib.pyplot as plt

# Загрузите данные истории обучения из файлов
file_paths = [
    '/content/unet_model_training_history.pkl',
    '/content/sa_unet_training_history.pkl',
    '/content/ra_unet_training_history.pkl',
    '/content/nuclei_segnet_training_history.pkl',
    '/content/ce_net_training_history.pkl',
    '/content/unetcbam_model_training_history.pkl',
]

model_names = ['deep_unet', 'sa_unet', 'ra_unet', 'nuclei_segnet', 'ce_net', 'unetcbam']

dice_coefficients = []  # Список для хранения метрики Dice Coefficient
best_dice_scores = []  # Список для хранения лучших оценок Dice Coefficient
best_epochs = []  # Список для хранения номеров эпох с лучшими оценками

for file_path in file_paths:
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
        dice_coefficient = history['dice_coef']  # Используем метрику Dice Coefficient
        max_dice_score = max(dice_coefficient)  # Находим максимальное значение Dice Coefficient
        best_epoch = dice_coefficient.index(max_dice_score)  # Находим номер эпохи с лучшей оценкой
        best_dice_scores.append(max_dice_score)
        best_epochs.append(best_epoch)
        dice_coefficients.append(dice_coefficient)

# Создайте стильный график метрики Dice Coefficient для каждой модели
plt.figure(figsize=(8, 6))
plt.style.use('seaborn-whitegrid')  # Используем стиль "seaborn-whitegrid"

for i, dice_coefficient in enumerate(dice_coefficients):
    plt.plot(dice_coefficient, label=f'{model_names[i]} (Best: {best_dice_scores[i]:.2f})', linewidth=2)
    plt.scatter(best_epochs[i], best_dice_scores[i], color='red', marker='o')

# Настройка параметров графика
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Dice Coefficient', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)

# Настройка цветов и стиля линий
colors = ['b', 'g', 'r', 'c', 'm', 'y']
for i, line in enumerate(plt.gca().get_lines()):
    line.set_color(colors[i])

# Настройка шрифта и цветов текста
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = 'Times New Roman'
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['axes.labelcolor'] = 'black'

# Отобразите график
plt.tight_layout()
plt.show()

import pickle
import matplotlib.pyplot as plt

file_paths = [
    '/content/unet_model_training_history.pkl',
    '/content/sa_unet_training_history.pkl',
    '/content/ra_unet_training_history.pkl',
    '/content/nuclei_segnet_training_history.pkl',
    '/content/ce_net_training_history.pkl',
    '/content/unetcbam_model_training_history.pkl',
]

model_names = ['deep_unet', 'sa_unet', 'ra_unet', 'nuclei_segnet', 'ce_net', 'unetcbam']

accuracies = []
best_accuracy_scores = []  # Список для хранения лучших оценок точности
best_epochs = []  # Список для хранения номеров эпох с лучшими оценками

for file_path in file_paths:
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
        accuracy = history['accuracy']
        max_accuracy_score = max(accuracy)  # Находим максимальное значение точности
        best_epoch = accuracy.index(max_accuracy_score)  # Находим номер эпохи с лучшей оценкой
        best_accuracy_scores.append(max_accuracy_score)
        best_epochs.append(best_epoch)
        accuracies.append(accuracy)

plt.figure(figsize=(8, 6))
plt.style.use('seaborn-whitegrid')

for i, accuracy in enumerate(accuracies):
    plt.plot(accuracy, label=f'{model_names[i]} (Best: {best_accuracy_scores[i]:.2f})', linewidth=2)
    plt.scatter(best_epochs[i], best_accuracy_scores[i], color='red', marker='o')

plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Accuracy', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)

# Настройка цветов и стиля линий
colors = ['b', 'g', 'r', 'c', 'm', 'y']
for i, line in enumerate(plt.gca().get_lines()):
    line.set_color(colors[i])

# Настройка шрифта и цветов текста
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = 'Times New Roman'
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['axes.labelcolor'] = 'black'

# Отобразите график
plt.tight_layout()
plt.show()

import pickle
import matplotlib.pyplot as plt

file_paths = [
    '/content/unet_model_training_history.pkl',
    '/content/sa_unet_training_history.pkl',
    '/content/ra_unet_training_history.pkl',
    '/content/nuclei_segnet_training_history.pkl',
    '/content/ce_net_training_history.pkl',
    '/content/unetcbam_model_training_history.pkl',
]

model_names = ['deep_unet', 'sa_unet', 'ra_unet', 'nuclei_segnet', 'ce_net', 'unetcbam']

sensitivities = []
best_sensitivity_scores = []  # Список для хранения лучших оценок чувствительности
best_epochs = []  # Список для хранения номеров эпох с лучшими оценками

for file_path in file_paths:
    with open(file_path, 'rb') as file:
        history = pickle.load(file)
        sensitivity = history['sensitivity']  # Используем метрику чувствительности
        max_sensitivity_score = max(sensitivity)  # Находим максимальное значение чувствительности
        best_epoch = sensitivity.index(max_sensitivity_score)  # Находим номер эпохи с лучшей оценкой
        best_sensitivity_scores.append(max_sensitivity_score)
        best_epochs.append(best_epoch)
        sensitivities.append(sensitivity)

plt.figure(figsize=(8, 6))
plt.style.use('seaborn-whitegrid')

for i, sensitivity in enumerate(sensitivities):
    plt.plot(sensitivity, label=f'{model_names[i]} (Best: {best_sensitivity_scores[i]:.2f})', linewidth=2)
    plt.scatter(best_epochs[i], best_sensitivity_scores[i], color='red', marker='o')

plt.xlabel('epochs', fontsize=14)
plt.ylabel('sensitivity', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)

# Настройка цветов и стиля линий
colors = ['b', 'g', 'r', 'c', 'm', 'y']
for i, line in enumerate(plt.gca().get_lines()):
    line.set_color(colors[i])

# Настройка шрифта и цветов текста
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = 'Times New Roman'
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['axes.labelcolor'] = 'black'

# Отобразите график
plt.tight_layout()
plt.show()



















"""transres_unet"""

!pip install keras_unet_collection
from keras_unet_collection import models
help(models.transunet_2d)

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPooling2D, UpSampling2D, Add, concatenate, Activation, BatchNormalization
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import BinaryAccuracy

# Определение оптимизатора
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

def res_block(inputs, filter_size):
    # First Conv2D layer
    cb1 = Conv2D(filter_size, (3, 3), padding='same', activation="relu", kernel_regularizer=l2(1e-4))(inputs)
    cb1 = BatchNormalization()(cb1)
    # Second Conv2D layer parallel to the first one
    cb2 = Conv2D(filter_size, (1, 1), padding='same', activation="relu", kernel_regularizer=l2(1e-4))(inputs)
    cb2 = BatchNormalization()(cb2)
    # Addition of cb1 and cb2
    add = Add()([cb1, cb2])

    return add

def res_path(inputs, filter_size, path_number):
    # Minimum one residual block for every res path
    skip_connection = res_block(inputs, filter_size)

    # Two serial residual blocks for res path 2
    if path_number == 2:
        skip_connection = res_block(skip_connection, filter_size)

    # Three serial residual blocks for res path 1
    elif path_number == 1:
        skip_connection = res_block(skip_connection, filter_size)
        skip_connection = res_block(skip_connection, filter_size)

    return skip_connection

def decoder_block(inputs, mid_channels, out_channels):
    conv_kwargs = dict(
        activation='relu',
        padding='same',
        kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-4),
        data_format='channels_last'
    )

    # Upsampling (linear) layer
    db = UpSampling2D(size=(2, 2))(inputs)
    # First conv2D layer
    db = Conv2D(mid_channels, 3, **conv_kwargs)(db)
    db = BatchNormalization()(db)
    # Second conv2D layer
    db = Conv2D(out_channels, 3, **conv_kwargs)(db)
    db = BatchNormalization()(db)

    return db

def TransResUNet(input_size=(256, 256, 3)):
    # Input
    inputs = Input(input_size)
    inp = inputs
    input_shape = input_size

    # Handling input channels
    if input_size[-1] < 3:
        inp = Conv2D(3, 1)(inputs)
        input_shape = (input_size[0], input_size[0], 3)
    else:
        inp = inputs
        input_shape = input_size

    encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)

    enc1 = encoder.get_layer(name='block1_conv1')(inp)
    enc1 = encoder.get_layer(name='block1_conv2')(enc1)
    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)
    enc2 = encoder.get_layer(name='block2_conv1')(enc2)
    enc2 = encoder.get_layer(name='block2_conv2')(enc2)
    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)
    enc3 = encoder.get_layer(name='block3_conv1')(enc3)
    enc3 = encoder.get_layer(name='block3_conv2')(enc3)
    enc3 = encoder.get_layer(name='block3_conv3')(enc3)

    center = MaxPooling2D(pool_size=(2, 2))(enc3)
    center = decoder_block(center, 512, 256)

    res_path3 = res_path(enc3, 128, 3)
    dec3 = concatenate([res_path3, center], axis=3)
    dec3 = decoder_block(dec3, 256, 64)

    res_path2 = res_path(enc2, 64, 2)
    dec2 = concatenate([res_path2, dec3], axis=3)
    dec2 = decoder_block(dec2, 128, 64)

    res_path1 = res_path(enc1, 32, 1)
    dec1 = concatenate([res_path1, dec2], axis=3)
    dec1 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(dec1)
    dec1 = ReLU()(dec1)

    out = Conv2D(1, 1)(dec1)
    out = Activation('sigmoid')(out)

    model = Model(inputs=[inputs], outputs=[out])

    return model
#
input_size = (256, 256, 3)
transres_unet = TransResUNet(input_size)
transres_unet.summary()

# Определите оптимизатор и функцию потерь
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
transres_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("transres_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_transres_unet = transres_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
transres_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_transres_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("transres_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("transres_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_transres_unet.history, history_file)

# Save the final trained weights
transres_unet.save_weights("transres_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

# Функция для визуализации снимков, масок и прогнозов с контуром маски
import os
import cv2
import matplotlib.pyplot as plt
import math
import matplotlib.colors as mcolors


# Создаем цветовую карту для маски с указанными цветами
mask_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])


def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(transres_unet, test_images[:50], test_masks[:50], mask_cmap=mask_cmap, num_samples=30)

import pickle
import numpy as np
import matplotlib.pyplot as plt

# Загрузка истории обучения
model_names = ["UNet", "UNet-CBAM", "RE-UNet", "Attention UNet", "Residual UNet"]
history_files = [
    "/content/unet_model_training_history.pkl",
    "/content/unetcbam_model_training_history.pkl",
    "/content/re_unet_training_history.pkl",
    "/content/AttentionUNet_model_training_history.pkl",
    "/content/res_unet_training_history.pkl"
]

# Создание фигуры
fig, ax = plt.subplots()

# Итерация по моделям и файлам истории
for filename, model_name in zip(history_files, model_names):
    with open(filename, 'rb') as file:
        history = pickle.load(file)
        sensitivity_data = history['sensitivity']  # Измените 'sensitivity' на название метрики чувствительности, если оно отличается
        num_points = len(sensitivity_data)
        x_smooth = np.linspace(0, num_points - 1, num_points * 50)
        y_smooth = np.interp(x_smooth, range(num_points), sensitivity_data)
        plt.plot(x_smooth, y_smooth, lw=2)

# Настройка параметров графика
ax.set_xlabel('Epochs')
ax.set_ylabel('Sensitivity')
ax.set_title('Sensitivity Comparison for Different Models')
ax.legend(model_names, loc='lower right')  # Перенес названия моделей в легенду и сместили вниз вправо
ax.grid(True)

# Показать график
plt.show()





"""**transres_unet**"""

!pip install keras_unet_collection
from keras_unet_collection import models
help(models.transunet_2d)

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPooling2D, UpSampling2D, Add, concatenate, Activation, BatchNormalization
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import BinaryAccuracy

# Определение оптимизатора
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

def res_block(inputs, filter_size):
    # First Conv2D layer
    cb1 = Conv2D(filter_size, (3, 3), padding='same', activation="relu", kernel_regularizer=l2(1e-4))(inputs)
    cb1 = BatchNormalization()(cb1)
    # Second Conv2D layer parallel to the first one
    cb2 = Conv2D(filter_size, (1, 1), padding='same', activation="relu", kernel_regularizer=l2(1e-4))(inputs)
    cb2 = BatchNormalization()(cb2)
    # Addition of cb1 and cb2
    add = Add()([cb1, cb2])

    return add

def res_path(inputs, filter_size, path_number):
    # Minimum one residual block for every res path
    skip_connection = res_block(inputs, filter_size)

    # Two serial residual blocks for res path 2
    if path_number == 2:
        skip_connection = res_block(skip_connection, filter_size)

    # Three serial residual blocks for res path 1
    elif path_number == 1:
        skip_connection = res_block(skip_connection, filter_size)
        skip_connection = res_block(skip_connection, filter_size)

    return skip_connection

def decoder_block(inputs, mid_channels, out_channels):
    conv_kwargs = dict(
        activation='relu',
        padding='same',
        kernel_initializer='he_normal',
        kernel_regularizer=l2(1e-4),
        data_format='channels_last'
    )

    # Upsampling (linear) layer
    db = UpSampling2D(size=(2, 2))(inputs)
    # First conv2D layer
    db = Conv2D(mid_channels, 3, **conv_kwargs)(db)
    db = BatchNormalization()(db)
    # Second conv2D layer
    db = Conv2D(out_channels, 3, **conv_kwargs)(db)
    db = BatchNormalization()(db)

    return db

def TransResUNet(input_size=(256, 256, 3)):
    # Input
    inputs = Input(input_size)
    inp = inputs
    input_shape = input_size

    # Handling input channels
    if input_size[-1] < 3:
        inp = Conv2D(3, 1)(inputs)
        input_shape = (input_size[0], input_size[0], 3)
    else:
        inp = inputs
        input_shape = input_size

    encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)

    enc1 = encoder.get_layer(name='block1_conv1')(inp)
    enc1 = encoder.get_layer(name='block1_conv2')(enc1)
    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)
    enc2 = encoder.get_layer(name='block2_conv1')(enc2)
    enc2 = encoder.get_layer(name='block2_conv2')(enc2)
    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)
    enc3 = encoder.get_layer(name='block3_conv1')(enc3)
    enc3 = encoder.get_layer(name='block3_conv2')(enc3)
    enc3 = encoder.get_layer(name='block3_conv3')(enc3)

    center = MaxPooling2D(pool_size=(2, 2))(enc3)
    center = decoder_block(center, 512, 256)

    res_path3 = res_path(enc3, 128, 3)
    dec3 = concatenate([res_path3, center], axis=3)
    dec3 = decoder_block(dec3, 256, 64)

    res_path2 = res_path(enc2, 64, 2)
    dec2 = concatenate([res_path2, dec3], axis=3)
    dec2 = decoder_block(dec2, 128, 64)

    res_path1 = res_path(enc1, 32, 1)
    dec1 = concatenate([res_path1, dec2], axis=3)
    dec1 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(dec1)
    dec1 = ReLU()(dec1)

    out = Conv2D(1, 1)(dec1)
    out = Activation('sigmoid')(out)

    model = Model(inputs=[inputs], outputs=[out])

    return model
#
input_size = (256, 256, 3)
transres_unet = TransResUNet(input_size)
transres_unet.summary()

# Определите оптимизатор и функцию потерь
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
transres_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("transres_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_transres_unet = transres_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
transres_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_transres_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("transres_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("transres_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_transres_unet.history, history_file)

# Save the final trained weights
transres_unet.save_weights("transres_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_transres_unet.history['loss'], label='Training Loss')
plt.plot(history_transres_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_transres_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_transres_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_transres_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_transres_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_transres_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_transres_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_transres_unet.history['precision'], label='Training Precision')
plt.plot(history_transres_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_transres_unet.history['recall'], label='Training Recall')
plt.plot(history_transres_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = transres_unet.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
import os
import cv2
import matplotlib.pyplot as plt
import math
import matplotlib.colors as mcolors


# Создаем цветовую карту для маски с указанными цветами
mask_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])


def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(transres_unet, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)

import gc

del transres_unet
gc.collect()

"""**sa_unet**"""

from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Activation, multiply, add, Dropout, BatchNormalization, SpatialDropout2D
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras import regularizers

def SalientAttentionBlock(f_maps, sal_ins, pool_maps, num_fmaps, dropout_rate):
    conv1_salins = Conv2D(128, (1, 1), activation='relu')(sal_ins)
    conv1_salins = BatchNormalization()(conv1_salins)

    conv1_fmaps = Conv2D(128, (1, 1), strides=(2, 2), activation='relu')(f_maps)
    conv1_fmaps = BatchNormalization()(conv1_fmaps)

    attn_add = add([conv1_fmaps, conv1_salins])

    conv_1d = Conv2D(128, (3, 3), activation='relu', padding='same')(attn_add)
    conv_1d = BatchNormalization()(conv_1d)

    conv_1d = Conv2D(128, (3, 3), activation='relu', padding='same')(conv_1d)
    conv_1d = BatchNormalization()(conv_1d)

    conv_1d = Conv2D(1, (1, 1), activation='relu')(conv_1d)
    conv_1d = BatchNormalization()(conv_1d)

    conv_nd = Conv2D(num_fmaps, (1, 1), activation='relu')(conv_1d)
    attn_act = Activation('sigmoid')(conv_nd)

    attn = multiply([attn_act, pool_maps])
    attn = BatchNormalization()(attn)

    return attn

def UNetBlock(in_fmaps, num_fmaps, dropout_rate, regularization_coeff=1e-4):
    conv1 = Conv2D(num_fmaps, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(regularization_coeff))(in_fmaps)
    conv1 = BatchNormalization()(conv1)

    conv1 = SpatialDropout2D(dropout_rate)(conv1)

    conv_out = Conv2D(num_fmaps, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(regularization_coeff))(conv1)
    conv_out = BatchNormalization()(conv_out)

    return conv_out

def Network(input_size=(256, 256, 3), dropout_rate=0.5):
    input1 = Input(shape=input_size)

    # Убран второй входной тензор для силы сжатия

    conv1 = UNetBlock(input1, 32, dropout_rate)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    # Убраны блоки, связанные с вторым входом

    conv2 = UNetBlock(pool1, 32, dropout_rate)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    # Убраны блоки, связанные с вторым входом

    conv3 = UNetBlock(pool2, 64, dropout_rate)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    # Убраны блоки, связанные с вторым входом

    conv4 = UNetBlock(pool3, 64, dropout_rate)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    # Убраны блоки, связанные с вторым входом

    conv5 = UNetBlock(pool4, 128, dropout_rate)

    up6 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv5), pool3], axis=3)
    conv6 = UNetBlock(up6, 64, dropout_rate)

    up7 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6), pool2], axis=3)
    conv7 = UNetBlock(up7, 64, dropout_rate)

    up8 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv7), pool1], axis=3)
    conv8 = UNetBlock(up8, 32, dropout_rate)

    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)
    conv9 = UNetBlock(up9, 32, dropout_rate)

    # Дополнительные блоки UNetBlock
    conv10 = UNetBlock(conv9, 32, dropout_rate)
    conv11 = UNetBlock(conv10, 32, dropout_rate)
    conv12 = UNetBlock(conv11, 32, dropout_rate)

    # Три дополнительных блока UNetBlock
    conv13 = UNetBlock(conv12, 32, dropout_rate)
    conv14 = UNetBlock(conv13, 32, dropout_rate)
    conv15 = UNetBlock(conv14, 32, dropout_rate)

    conv_final = Conv2D(1, (1, 1), activation='sigmoid')(conv15)  # Финальный слой

    model = Model(inputs=input1, outputs=conv_final)

    return model

# Создаем модель с новыми размерами, слоями Dropout, BatchNormalization и регуляризацией
input_size = (256, 256, 3)
dropout_rate = 0.5
sa_unet = Network(input_size, dropout_rate)
sa_unet.summary()

# Определите оптимизатор и функцию потерь
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
sa_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History

# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("sa_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_sa_unet = sa_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
sa_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_sa_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("sa_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("sa_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_transres_unet.history, history_file)

# Save the final trained weights
sa_unet.save_weights("sa_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_sa_unet.history['loss'], label='Training Loss')
plt.plot(history_sa_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_sa_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_sa_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_sa_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_sa_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_sa_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_sa_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_sa_unet.history['precision'], label='Training Precision')
plt.plot(history_sa_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_sa_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_sa_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_sa_unet.history['recall'], label='Training Recall')
plt.plot(history_sa_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = sa_unet.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
import os
import cv2
import matplotlib.pyplot as plt
import math
import matplotlib.colors as mcolors


# Создаем цветовую карту для маски с указанными цветами
mask_cmap = mcolors.ListedColormap(['#2E0854', 'yellow'])


def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(sa_unet, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)

import gc

del sa_unet
gc.collect()

"""**CBAM-UNet**"""

import tensorflow as tf
from tensorflow.keras.layers import Add, Dense, Conv2D, Concatenate, MaxPooling2D, Activation, GlobalAveragePooling2D, GlobalMaxPooling2D
from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D, Dropout, BatchNormalization, Reshape, multiply
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Lambda, Permute, Concatenate

'''
U-Net: Convolutional Networks for Biomedical Image Segmentation
(https://arxiv.org/abs/1505.04597)
CBAM: Convolutional Block Attention Module
(https://arxiv.org/pdf/1807.06521.pdf)
---
img_shape: (height, width, channels)
out_ch: number of output channels
start_ch: number of channels of the first conv
depth: zero indexed depth of the U-structure
inc_rate: rate at which the conv channels will increase
activation: activation function after convolutions
dropout: amount of dropout in the contracting part
res_rate: rate at which the residual blocks repeat
ratio: ratio of MLP with one hidden layer
batchnorm: adds Batch Normalization if true
maxpool: use strided conv instead of maxpooling if false
upconv: use transposed conv instead of upsamping + conv if false
residual: add residual connections around each attention block if true
dilated: change convolution to dilate convolution for fast train if true
'''


def get_unetcbam_model(input_channel_num=3, out_ch=3, start_ch=16, depth=5, inc_rate=2., activation='relu',
                       dropout=0.5, ratio=4, batchnorm=True, maxpool=False, upconv=True, residual=False, dilated=False):
    def _channel_attention(m, out_ch, ratio):
        avg_pool = GlobalAveragePooling2D()(m)
        avg_pool = Reshape((1, 1, out_ch))(avg_pool)
        avg_pool = Dense(units=out_ch // ratio)(avg_pool)
        avg_pool = Activation('relu')(avg_pool)
        avg_pool = Dense(units=out_ch)(avg_pool)

        max_pool = GlobalMaxPooling2D()(m)
        max_pool = Reshape((1, 1, out_ch))(max_pool)
        max_pool = Dense(units=out_ch // ratio)(max_pool)
        max_pool = Activation('relu')(max_pool)
        max_pool = Dense(units=out_ch)(max_pool)

        channel_attention = Add()([avg_pool, max_pool])
        channel_attention = Activation('sigmoid')(channel_attention)

        return multiply([m, channel_attention])

    def _spatial_attention(m, out_ch, ratio, kernel_size=7):
        if tf.keras.backend.image_data_format() == "channels_first":
            spatial_attention = Permute((2, 3, 1))(m)
        else:
            spatial_attention = m

        avg_pool = Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True))(spatial_attention)
        max_pool = Lambda(lambda x: tf.keras.backend.max(x, axis=3, keepdims=True))(spatial_attention)
        concat = Concatenate(axis=3)([avg_pool, max_pool])
        spatial_attention = Conv2D(1, kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal',
                                   use_bias=False)(concat)

        if tf.keras.backend.image_data_format() == "channels_first":
            spatial_attention = Permute((3, 1, 2))(spatial_attention)

        return multiply([m, spatial_attention])

    def _conv_block(m, dim, acti, bn, di, res, do=0):
        n = Conv2D(dim, 3, padding='same', dilation_rate=2)(m) if di else Conv2D(dim, 3, padding='same')(m)
        n = BatchNormalization()(n) if bn else n
        n = Activation(acti)(n)
        n = Dropout(do)(n) if do else n
        n = Conv2D(dim, 3, padding='same', dilation_rate=2)(n) if di else Conv2D(dim, 3, padding='same')(n)
        n = BatchNormalization()(n) if bn else n
        n = Activation(acti)(n)
        n = Add()([n, _channel_attention(n, dim, ratio)]) if res else _channel_attention(n, dim, ratio)
        n = Add()([n, _spatial_attention(n, dim, ratio)]) if res else _spatial_attention(n, dim, ratio)

        return n

    def _level_block(m, dim, depth, inc, acti, do, bn, mp, up, res, di):
        if depth > 0:
            n = _conv_block(m, dim, acti, bn, di, res)
            m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)
            m = _level_block(m, int(inc * dim), depth - 1, inc, acti, do, bn, mp, up, res, di)
            if up:
                m = UpSampling2D()(m)
                m = Conv2D(dim, 2, activation=acti, padding='same')(m)
            else:
                m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)
            n = Concatenate()([n, m])
            m = _conv_block(n, dim, acti, bn, di, res)
        else:
            m = _conv_block(m, dim, acti, bn, di, res, do)

        return m

    i = Input(shape=(None, None, input_channel_num))
    o = _level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual, dilated)
    o = Conv2D(1, 1, activation='sigmoid')(o)

    model = Model(inputs=i, outputs=o)

    return model

# Компилируйте модель
optimizer = Adam(learning_rate=0.001)  # Выберите желаемую скорость обучения
unetcbam_model = get_unetcbam_model()

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
unetcbam_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

# Выведите информацию о модели
unetcbam_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("unetcbam_model_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_unetcbam_model = unetcbam_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
unetcbam_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_unetcbam_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("unetcbam_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("unetcbam_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_unetcbam_model.history, history_file)

# Save the final trained weights
unetcbam_model.save_weights("unetcbam_model_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_unetcbam_model.history['loss'], label='Training Loss')
plt.plot(history_unetcbam_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_unetcbam_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_unetcbam_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unetcbam_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unetcbam_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_unetcbam_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_unetcbam_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_unetcbam_model.history['precision'], label='Training Precision')
plt.plot(history_unetcbam_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_unetcbam_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_unetcbam_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_unetcbam_model.history['recall'], label='Training Recall')
plt.plot(history_unetcbam_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = unetcbam_model.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(unetcbam_model, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)

import gc

del unetcbam_model
gc.collect()

"""**ra_unet**"""

from tensorflow.keras import models, layers, regularizers
from tensorflow.keras import backend as K

# Слой для получения сигнала врат (gating signal)
def gatingsignal(input, out_size, batchnorm=False):
    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x

# Attention блок
def attention_block(x, gating, inter_shape):
    shape_x = K.int_shape(x)
    shape_g = K.int_shape(gating)
    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), kernel_initializer='he_normal', padding='same')(x)
    shape_theta_x = K.int_shape(theta_x)
    phi_g = layers.Conv2D(inter_shape, (1, 1), kernel_initializer='he_normal', padding='same')(gating)
    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3), strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]), kernel_initializer='he_normal', padding='same')(phi_g)
    concat_xg = layers.add([upsample_g, theta_x])
    act_xg = layers.Activation('relu')(concat_xg)
    psi = layers.Conv2D(1, (1, 1), kernel_initializer='he_normal', padding='same')(act_xg)
    sigmoid_xg = layers.Activation('sigmoid')(psi)
    shape_sigmoid = K.int_shape(sigmoid_xg)
    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)
    upsample_psi = layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': shape_x[3]})(upsample_psi)
    y = layers.multiply([upsample_psi, x])
    result = layers.Conv2D(shape_x[3], (1, 1), kernel_initializer='he_normal', padding='same')(y)
    attenblock = layers.BatchNormalization()(result)
    return attenblock

# Residual блок
def res_conv_block(x, kernelsize, filters, dropout, batchnorm=False):
    conv1 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(x)
    if batchnorm:
        conv1 = layers.BatchNormalization(axis=3)(conv1)
    conv1 = layers.Activation('relu')(conv1)
    conv2 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(conv1)
    if batchnorm:
        conv2 = layers.BatchNormalization(axis=3)(conv2)
        conv2 = layers.Activation("relu")(conv2)
    if dropout > 0:
        conv2 = layers.Dropout(dropout)(conv2)
    # Skip connection
    shortcut = layers.Conv2D(filters, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(x)
    if batchnorm:
        shortcut = layers.BatchNormalization(axis=3)(shortcut)
    shortcut = layers.Activation("relu")(shortcut)
    respath = layers.add([shortcut, conv2])
    return respath

# Residual-Attention U-Net (RA-UNET)
def residual_attentionunet(input_shape, dropout=0.2, batchnorm=True):
    filters = [16, 32, 64, 128, 256]
    kernelsize = 3
    upsample_size = 2

    inputs = layers.Input(input_shape)

    # Downsampling layers
    dn_1 = res_conv_block(inputs, kernelsize, filters[0], dropout, batchnorm)
    pool1 = layers.MaxPooling2D(pool_size=(2,2))(dn_1)

    dn_2 = res_conv_block(pool1, kernelsize, filters[1], dropout, batchnorm)
    pool2 = layers.MaxPooling2D(pool_size=(2,2))(dn_2)

    dn_3 = res_conv_block(pool2, kernelsize, filters[2], dropout, batchnorm)
    pool3 = layers.MaxPooling2D(pool_size=(2,2))(dn_3)

    dn_4 = res_conv_block(pool3, kernelsize, filters[3], dropout, batchnorm)
    pool4 = layers.MaxPooling2D(pool_size=(2,2))(dn_4)

    dn_5 = res_conv_block(pool4, kernelsize, filters[4], dropout, batchnorm)

    # Upsampling layers
    gating_5 = gatingsignal(dn_5, filters[3], batchnorm)
    att_5 = attention_block(dn_4, gating_5, filters[3])
    up_5 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(dn_5)
    up_5 = layers.concatenate([up_5, att_5], axis=3)
    up_conv_5 = res_conv_block(up_5, kernelsize, filters[3], dropout, batchnorm)

    gating_4 = gatingsignal(up_conv_5, filters[2], batchnorm)
    att_4 = attention_block(dn_3, gating_4, filters[2])
    up_4 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(up_conv_5)
    up_4 = layers.concatenate([up_4, att_4], axis=3)
    up_conv_4 = res_conv_block(up_4, kernelsize, filters[2], dropout, batchnorm)

    gating_3 = gatingsignal(up_conv_4, filters[1], batchnorm)
    att_3 = attention_block(dn_2, gating_3, filters[1])
    up_3 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(up_conv_4)
    up_3 = layers.concatenate([up_3, att_3], axis=3)
    up_conv_3 = res_conv_block(up_3, kernelsize, filters[1], dropout, batchnorm)

    gating_2 = gatingsignal(up_conv_3, filters[0], batchnorm)
    att_2 = attention_block(dn_1, gating_2, filters[0])
    up_2 = layers.UpSampling2D(size=(upsample_size, upsample_size), data_format="channels_last")(up_conv_3)
    up_2 = layers.concatenate([up_2, att_2], axis=3)
    up_conv_2 = res_conv_block(up_2, kernelsize, filters[0], dropout, batchnorm)

    conv_final = layers.Conv2D(1, kernel_size=(1,1))(up_conv_2)
    conv_final = layers.BatchNormalization(axis=3)(conv_final)
    outputs = layers.Activation('sigmoid')(conv_final)

    model = models.Model(inputs=[inputs], outputs=[outputs])
    model.summary()
    return model

image_height = 256
image_width = 256
num_image_channels = 3
num_mask_channels = 1

ra_unet = residual_attentionunet(input_shape=(image_height, image_width, num_image_channels), dropout=0.2, batchnorm=True)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
ra_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])

# Print the summary of the CE-Net model
ra_unet.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
checkpoint_callback = ModelCheckpoint("ra_unet_best_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_ra_unet = ra_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
ra_unet_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_ra_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("ra_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("ra_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_ra_unet.history, history_file)

# Save the final trained weights
ra_unet.save_weights("ra_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_ra_unet.history['loss'], label='Training Loss')
plt.plot(history_ra_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_ra_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_ra_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_ra_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_ra_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_ra_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_ra_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_ra_unet.history['precision'], label='Training Precision')
plt.plot(history_ra_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_ra_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_ra_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_ra_unet.history['recall'], label='Training Recall')
plt.plot(history_ra_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = ra_unet.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(ra_unet, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)

import gc

del ra_unet
gc.collect()

"""**dense_inception_unet**"""

from tensorflow.keras.layers import Layer, Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Input
from tensorflow.keras.models import Model

def dense_inception_module(X, filters):
    c1 = Conv2D(filters, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(X)

    c2 = Conv2D(filters, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(X)
    c2 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(c2)

    c3 = Conv2D(filters, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(X)
    c3 = Conv2D(filters, kernel_size=5, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(c3)

    c4 = MaxPooling2D(pool_size=(4, 4), strides=1, padding='same')(X)
    c4 = Conv2D(filters, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(c4)

    concat = concatenate([c1, c2, c3, c4], axis=-1)
    return concat

class DenseInceptionUNet(Model):
    def __init__(self, input_shape):
        super(DenseInceptionUNet, self).__init__()
        self.input_layer = Input(shape=input_shape)

        self.encoder1 = dense_inception_module(self.input_layer, 32)
        self.pool1 = MaxPooling2D(pool_size=(4, 4))(self.encoder1)
        self.encoder2 = dense_inception_module(self.pool1, 64)
        self.pool2 = MaxPooling2D(pool_size=(4, 4))(self.encoder2)
        self.encoder3 = dense_inception_module(self.pool2, 128)
        self.pool3 = MaxPooling2D(pool_size=(4, 4))(self.encoder3)
        self.encoder4 = dense_inception_module(self.pool3, 256)
        self.pool4 = MaxPooling2D(pool_size=(4, 4))(self.encoder4)

        self.bridge = dense_inception_module(self.pool4, 512)

        # Additional layers in the decoder
        self.decoder5 = dense_inception_module(self.bridge, 256)

        self.up1 = UpSampling2D(size=(4, 4))(self.decoder5)
        self.concat1 = concatenate([self.up1, self.encoder4], axis=-1)
        self.decoder1 = dense_inception_module(self.concat1, 256)
        self.up2 = UpSampling2D(size=(4, 4))(self.decoder1)
        self.concat2 = concatenate([self.up2, self.encoder3], axis=-1)
        self.decoder2 = dense_inception_module(self.concat2, 128)
        self.up3 = UpSampling2D(size=(4, 4))(self.decoder2)
        self.concat3 = concatenate([self.up3, self.encoder2], axis=-1)
        self.decoder3 = dense_inception_module(self.concat3, 64)
        self.up4 = UpSampling2D(size=(4, 4))(self.decoder3)
        self.concat4 = concatenate([self.up4, self.encoder1], axis=-1)
        self.decoder4 = dense_inception_module(self.concat4, 32)

        self.output_layer = Conv2D(1, kernel_size=1, activation='sigmoid')(self.decoder4)

        # Create a Model instance
        self.model = Model(inputs=self.input_layer, outputs=self.output_layer)

    def call(self, X):
        return self.model(X)

# Define the input shape
input_shape = (256, 256, 3)

# Create an instance of the DenseInceptionUNet model
dense_inception_unet = DenseInceptionUNet(input_shape)

# Определите оптимизатор и функцию потерь
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy

# Определите оптимизатор и функцию потерь
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
dense_inception_unet.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])


# Print the model summary
dense_inception_unet.model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
#checkpoint_callback = ModelCheckpoint("dense_inception_unet_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_dense_inception_unet = dense_inception_unet.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    #callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_ra_unet.history)

# Save the DataFrame to a CSV file
history_df.to_csv("dense_inception_unet_training_history.csv", index=False)

# Save the training history as a pickle file
with open("dense_inception_unet_training_history.pkl", "wb") as history_file:
    pickle.dump(history_dense_inception_unet.history, history_file)

# Save the final trained weights
#dense_inception_unet.save_weights("dense_inception_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_dense_inception_unet.history['loss'], label='Training Loss')
plt.plot(history_dense_inception_unet.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_dense_inception_unet.history['accuracy'], label='Training Accuracy')
plt.plot(history_dense_inception_unet.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_dense_inception_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_dense_inception_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_dense_inception_unet.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_dense_inception_unet.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_dense_inception_unet.history['precision'], label='Training Precision')
plt.plot(history_dense_inception_unet.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_dense_inception_unet.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_dense_inception_unet.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_dense_inception_unet.history['recall'], label='Training Recall')
plt.plot(history_dense_inception_unet.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = dense_inception_unet.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(dense_inception_unet, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)

import gc

del dense_inception_unet
gc.collect()

"""**RES-UNet**"""

# Building Unet by dividing encoder and decoder into blocks

import tensorflow as tf
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda
# from tf.keras.optimizers import Adam
from keras.layers import Activation, MaxPool2D, Concatenate


def conv_block(input, num_filters):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)   #Not in the original network.
    x = Activation("relu")(x)

    x1 = Conv2D(num_filters, 3, padding="same")(x)
    x1 = BatchNormalization()(x1)  #Not in the original network
    x1 = Activation("relu")(x1)
    x2 =  x + x1

    x = Conv2D(num_filters, 3, padding="same")(x2)
    return x

#Encoder block: Conv block followed by maxpooling


def encoder_block(input, num_filters):
    x = conv_block(input, num_filters)
    p = MaxPool2D((2, 2))(x)
    return x, p

#Decoder block
#skip features gets input from encoder for concatenation

def decoder_block(input, skip_features, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, num_filters)
    return x

#Build Unet using the blocks
def build_resunet(input_shape, n_classes):
    inputs = Input(input_shape)

    s1, p1 = encoder_block(inputs, 32)
    s2, p2 = encoder_block(p1, 64)
    s3, p3 = encoder_block(p2, 128)
    s4, p4 = encoder_block(p3, 256)

    b1 = conv_block(p4, 512) #Bridge

    d1 = decoder_block(b1, s4, 256)
    d2 = decoder_block(d1, s3, 128)
    d3 = decoder_block(d2, s2, 64)
    d4 = decoder_block(d3, s1, 32)

    if n_classes == 1:  #Binary
      activation = 'sigmoid'
    else:
      activation = 'softmax'

    outputs = Conv2D(1, 1, padding="same", activation=activation)(d4)  #Change the activation based on n_classes
    print(activation)


    model = Model(inputs, outputs, name="Res-UNet")

    return model
from keras.optimizers import Adam

# Define input shape and number of classes
input_shape = (256, 256, 3)
num_classes = 1  # Assuming it's binary segmentation, change it for multi-class

# Build the Res-UNet model
resunet_model = build_resunet(input_shape, num_classes)


optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
resunet_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])


# Print the model summary
resunet_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
#checkpoint_callback = ModelCheckpoint("dense_inception_unet_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_resunet_model = resunet_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    #callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
resunet_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_resunet_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("resunet_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("resunet_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_resunet_model.history, history_file)

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_resunet_model.history['loss'], label='Training Loss')
plt.plot(history_resunet_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_resunet_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_resunet_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_resunet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_resunet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_resunet_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_resunet_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_resunet_model.history['precision'], label='Training Precision')
plt.plot(history_resunet_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_resunet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_resunet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_resunet_model.history['recall'], label='Training Recall')
plt.plot(history_resunet_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = resunet_model.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(resunet_model, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)

import gc

del resunet_model
gc.collect()

"""**Attention_UNet**"""

from tensorflow.keras.layers import Layer, Conv2D, Dropout, MaxPool2D, UpSampling2D, concatenate, Add, Multiply, BatchNormalization, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.metrics import Accuracy

class EncoderBlock(Layer):
    def __init__(self, filters, rate, pooling=True, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.filters = filters
        self.rate = rate
        self.pooling = pooling

        self.c1 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')
        self.c2 = Conv2D(filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')
        self.drop = Dropout(rate)
        self.pool = MaxPool2D()

    def call(self, X):
        x = self.c1(X)
        x = self.c2(x)
        x = self.drop(x)
        if self.pooling:
            y = self.pool(x)
            return y, x
        else:
            return x

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "rate": self.rate,
            "pooling": self.pooling
        }

class DecoderBlock(Layer):
    def __init__(self, filters, rate, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.filters = filters
        self.rate = rate

        self.up = UpSampling2D()
        self.net = EncoderBlock(filters, rate, pooling=False)

    def call(self, X):
        X, skip_X = X
        x = self.up(X)
        c_ = concatenate([x, skip_X])
        x = self.net(c_)
        return x

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "rate": self.rate
        }

class AttentionGate(Layer):
    def __init__(self, filters, bn, **kwargs):
        super(AttentionGate, self).__init__(**kwargs)
        self.filters = filters
        self.bn = bn

        self.normal = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal')
        self.down = Conv2D(filters, kernel_size=3, strides=2, padding='same', activation='relu', kernel_initializer='he_normal')
        self.learn = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')
        self.resample = UpSampling2D()
        self.BN = BatchNormalization()

    def call(self, X):
        X, skip_X = X

        x = self.normal(X)
        skip = self.down(skip_X)
        x = Add()([x, skip])
        x = self.learn(x)
        x = self.resample(x)
        f = Multiply()([x, skip_X])
        if self.bn:
            return self.BN(f)
        else:
            return f

    def get_config(self):
        base_config = super().get_config()
        return {
            **base_config,
            "filters": self.filters,
            "bn": self.bn
        }

# Inputs
input_layer = Input(shape=(256, 256, 3))

# Encoder
p1, c1 = EncoderBlock(64, 0.1, name="Encoder1")(input_layer)
p2, c2 = EncoderBlock(128, 0.1, name="Encoder2")(p1)
p3, c3 = EncoderBlock(256, 0.2, name="Encoder3")(p2)
p4, c4 = EncoderBlock(512, 0.2, name="Encoder4")(p3)

# Encoding
encoding = EncoderBlock(512, 0.3, pooling=False, name="Encoding")(p4)

# Attention + Decoder

a1 = AttentionGate(256, bn=True, name="Attention1")([encoding, c4])
d1 = DecoderBlock(256, 0.2, name="Decoder1")([encoding, a1])

a2 = AttentionGate(128, bn=True, name="Attention2")([d1, c3])
d2 = DecoderBlock(128, 0.2, name="Decoder2")([d1, a2])

a3 = AttentionGate(64, bn=True, name="Attention3")([d2, c2])
d3 = DecoderBlock(64, 0.1, name="Decoder3")([d2, a3])

a4 = AttentionGate(32, bn=True, name="Attention4")([d3, c1])
d4 = DecoderBlock(32, 0.1, name="Decoder4")([d3, a4])

# Output
output_layer = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(d4)

# Model
AttentionUNet_model = Model(inputs=[input_layer], outputs=[output_layer])

# Define the optimizer and loss function
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = tf.keras.losses.BinaryCrossentropy()

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Выберите параметры оптимизатора
AttentionUNet_model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy', dice_coef, f1_score, recall, precision, mean_iou, sensitivity])


# Print the model summary
AttentionUNet_model.summary()

import time
import pickle
import pandas as pd
from keras.callbacks import ModelCheckpoint, History


# Define a callback to save model checkpoints
#checkpoint_callback = ModelCheckpoint("dense_inception_unet_weights.h5", monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# Define a callback to record training history
history_callback = History()

# Record the start time
start_time = time.time()

# Training
history_AttentionUNet_model = AttentionUNet_model.fit(
    train_images,  # Provide your training images here
    train_masks,  # Provide your training masks here
    batch_size=8,  # Adjust batch size as needed
    epochs=120,  # Adjust the number of epochs as needed
    validation_split=0.2,  # Validation split
    verbose=1,  # Set verbosity level as needed
    #callbacks=[checkpoint_callback, history_callback]  # Add callbacks to save checkpoints and history
)

end_time = time.time()
AttentionUNet_model_model_time = end_time - start_time

# Calculate the training duration
training_duration = end_time - start_time

# Save the training history to a DataFrame
history_df = pd.DataFrame(history_AttentionUNet_model.history)

# Save the DataFrame to a CSV file
history_df.to_csv("AttentionUNet_model_training_history.csv", index=False)

# Save the training history as a pickle file
with open("AttentionUNet_model_training_history.pkl", "wb") as history_file:
    pickle.dump(history_AttentionUNet_model.history, history_file)

# Save the final trained weights
#dense_inception_unet.save_weights("dense_inception_unet_final_weights.h5")

# Print training duration
print(f"Training Duration: {training_duration:.2f} seconds")

plt.figure(figsize=(18, 12))  # Увеличиваем размер фигуры для размещения большего количества графиков

# Loss
plt.subplot(2, 4, 1)
plt.plot(history_AttentionUNet_model.history['loss'], label='Training Loss')
plt.plot(history_AttentionUNet_model.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(2, 4, 2)
plt.plot(history_AttentionUNet_model.history['accuracy'], label='Training Accuracy')
plt.plot(history_AttentionUNet_model.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_AttentionUNet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_AttentionUNet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Dice Coefficient
plt.subplot(2, 4, 4)
plt.plot(history_AttentionUNet_model.history['dice_coef'], label='Training Dice Coefficient')
plt.plot(history_AttentionUNet_model.history['val_dice_coef'], label='Validation Dice Coefficient')
plt.xlabel('Epochs')
plt.ylabel('Dice Coefficient')
plt.legend()

# Precision
plt.subplot(2, 4, 6)
plt.plot(history_AttentionUNet_model.history['precision'], label='Training Precision')
plt.plot(history_AttentionUNet_model.history['val_precision'], label='Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Sensitivity
plt.subplot(2, 4, 3)
plt.plot(history_AttentionUNet_model.history['sensitivity'], label='Training Sensitivity')
plt.plot(history_AttentionUNet_model.history['val_sensitivity'], label='Validation Sensitivity')
plt.xlabel('Epochs')
plt.ylabel('Sensitivity')
plt.legend()

# Recall
plt.subplot(2, 4, 7)
plt.plot(history_AttentionUNet_model.history['recall'], label='Training Recall')
plt.plot(history_AttentionUNet_model.history['val_recall'], label='Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

plt.tight_layout()  # Обеспечиваем, чтобы подграфики не перекрывались
plt.show()

# Оцените модель на тестовых данных
test_metrics = AttentionUNet_model.evaluate(test_images, test_masks)

# Выведите метрики
print(f'Test Loss: {test_metrics[0]}')
print(f'Test Accuracy: {test_metrics[1]}')
print(f'Test Dice: {test_metrics[2]}')
print(f'Mean IoU: {test_metrics[3]}')
print(f'Precision: {test_metrics[4]}')
print(f'Recall: {test_metrics[5]}')
print(f'Sensitivity: {test_metrics[6]}')

# Функция для визуализации снимков, масок и прогнозов с контуром маски
def visualize_predictions(model, images, masks, mask_cmap=None, num_samples=5):
    num_samples = min(num_samples, len(images))

    # Выбираем первые 30 снимков и масок
    sample_indices = range(num_samples)

    for i, idx in enumerate(sample_indices):
        image = images[idx]
        mask = masks[idx]

        # Прогноз с использованием модели
        prediction = model.predict(np.expand_dims(image, axis=0))[0]

        # Создаем изображение с контурами маски
        mask_contours = np.zeros_like(mask, dtype=np.uint8)
        contours, _ = cv2.findContours(mask.squeeze().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mask_contours, contours, -1, (255), thickness=1)

        # Отображаем снимок
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 4, 1)
        plt.imshow(image)
        plt.title("Image")
        plt.axis('off')

        # Отображаем маску
        plt.subplot(1, 4, 2)
        if mask_cmap:
            plt.imshow(mask.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(mask.squeeze(), cmap='gray')
        plt.title("Mask")
        plt.axis('off')

        # Отображаем прогноз
        plt.subplot(1, 4, 3)
        if mask_cmap:
            plt.imshow(prediction.squeeze(), cmap=mask_cmap)
        else:
            plt.imshow(prediction.squeeze(), cmap='gray')
        plt.title("Prediction")
        plt.axis('off')

        # Отображаем контур маски
        plt.subplot(1, 4, 4)
        plt.imshow(mask_contours.squeeze(), cmap='gray')
        plt.title("Mask Contour")
        plt.axis('off')

        plt.show()

# Визуализируем первые 30 снимков, масок и прогнозов
visualize_predictions(AttentionUNet_model, test_images[:30], test_masks[:30], mask_cmap=mask_cmap, num_samples=30)





import pickle
import matplotlib.pyplot as plt

# Define the model names
model_names = [
    'unet',
    'unet_cbam',
    'ra-unet',
    'res_unet',
    'att_unet',

]

# Define a dictionary to store the histories
histories = {}

# Iterate through the provided filenames
for filename, model_name in zip([
    "/content/Новая папка_до аугмента/unet_training_history.pkl",
    "/content/Новая папка_до аугмента/model_resunet_training_history.pkl",
    "/content/Новая папка_до аугмента/dense_inception_unet_training_history.pkl",
    "/content/Новая папка_до аугмента/UnetPlusPlus_model_training_history.pkl",
    "/content/Новая папка_до аугмента/AttentionUNet_model_training_history.pkl"
], model_names):
    with open(filename, 'rb') as file:
        history = pickle.load(file)
        histories[model_name] = history
        plt.plot(history['dice_coef'], label=model_name, lw=2)  # Set line width

# Configure plot settings
plt.xlabel('Epochs')
plt.ylabel('Dice coefficient')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

import pickle
import matplotlib.pyplot as plt

# Define the model names
model_names = [
    'unet',
    'unet_cbam',
    'ra-unet',
    'res_unet',
    'att_unet',

]
# Define a dictionary to store the histories
histories = {}

# Iterate through the provided filenames
for filename, model_name in zip([
    "/content/Новая папка_до аугмента/unet_training_history.pkl",
    "/content/Новая папка_до аугмента/model_resunet_training_history.pkl",
    "/content/Новая папка_до аугмента/dense_inception_unet_training_history.pkl",
    "/content/Новая папка_до аугмента/UnetPlusPlus_model_training_history.pkl",
    "/content/Новая папка_до аугмента/AttentionUNet_model_training_history.pkl"
], model_names):
    with open(filename, 'rb') as file:
        history = pickle.load(file)
        histories[model_name] = history
        plt.plot(history['loss'], label=model_name, lw=2)  # Set line width

# Configure plot settings
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Comparison for Different Models')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

import pickle
import matplotlib.pyplot as plt

model_names = [
    'unet',
    'unet_cbam',
    'ra-unet',
    'res_unet',
    'att_unet',

]
# Load dice_coef values from the history files
dice_coef_values = []
for filename in [
    "/content/Новая папка_до аугмента/unet_training_history.pkl",
    "/content/Новая папка_до аугмента/model_resunet_training_history.pkl",
    "/content/Новая папка_до аугмента/dense_inception_unet_training_history.pkl",
    "/content/Новая папка_до аугмента/UnetPlusPlus_model_training_history.pkl",
    "/content/Новая папка_до аугмента/AttentionUNet_model_training_history.pkl"
]:
    with open(filename, 'rb') as file:
        history = pickle.load(file)
        dice_coef_values.append(history['dice_coef'])

# Custom colors for each box
colors = ['#1f77b4', '#d62728', '#2ca02c', '#e377c2', '#ff7f0e']

# Create a box plot
plt.figure(figsize=(12, 8))
boxplot = plt.boxplot(dice_coef_values, showfliers=False, patch_artist=True, labels=model_names)

plt.ylabel('Dice Coefficient', fontsize=14)
plt.xticks(fontsize=14, rotation=45)
plt.yticks(fontsize=14)

# Customizing box colors
for box, color in zip(boxplot['boxes'], colors):
    box.set_facecolor(color)

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import pickle
import pandas as pd
import numpy as np

# List of history file paths
history_files = [
    "/content/Новая папка_до аугмента/unet_training_history.pkl",
    "/content/Новая папка_до аугмента/model_resunet_training_history.pkl",
    "/content/Новая папка_до аугмента/dense_inception_unet_training_history.pkl",
    "/content/Новая папка_до аугмента/UnetPlusPlus_model_training_history.pkl",
    "/content/Новая папка_до аугмента/AttentionUNet_model_training_history.pkl"
]

# Define custom model names
model_names = [
    'unet',
    'unet_cbam',
    'ra-unet',
    'res_unet',
    'att_unet'
]

# Initialize lists to store metrics
train_metrics = []
val_metrics = []

# Custom function to compute F1 Score
def f1_score(precision, recall):
    return 2 * (precision * recall) / (precision + recall + 1e-15)

# Load history files and compute metrics
for file_path, model_name in zip(history_files, model_names):
    with open(file_path, 'rb') as file:
        history = pickle.load(file)

        if 'val_accuracy' not in history:
            continue  # Skip models with missing validation data

        train_metric = {
            'Model': model_name,
            'Accuracy': history['accuracy'][-1],
            'Dice Coefficient': history['dice_coef'][-1],
            'F1 Score': f1_score(history['precision'][-1], history['recall'][-1]),
            'Recall': history['recall'][-1],
            'Precision': history['precision'][-1],
            'Mean IoU': np.mean(history['mean_iou'])
        }

        val_metric = {
            'Model': model_name,
            'Accuracy': history['val_accuracy'][-1],
            'Dice Coefficient': history['val_dice_coef'][-1],
            'F1 Score': f1_score(history['val_precision'][-1], history['val_recall'][-1]),
            'Recall': history['val_recall'][-1],
            'Precision': history['val_precision'][-1],
            'Mean IoU': np.mean(history['val_mean_iou'])
        }

        train_metrics.append(train_metric)
        val_metrics.append(val_metric)

# Create DataFrames for training and validation metrics
train_df = pd.DataFrame(train_metrics)
val_df = pd.DataFrame(val_metrics)

# Print the DataFrames
print("Training Metrics:")
print(train_df)
print("\nValidation Metrics:")
print(val_df)

from google.colab import drive
drive.mount('/content/drive')











import shutil

# Указать пути к файлам
source_paths = [
    "/content/vgg16_unet_training_history.pkl",
    "/content/sa_unet_training_history.pkl"
]

# Указать путь к целевой папке на Google Drive
target_folder = "/content/drive/MyDrive/ТНБЦ кансер"

# Переместить файлы
for source_path in source_paths:
    shutil.move(source_path, target_folder)

print("Файлы успешно перемещены на Google Drive.")























!kaggle datasets download -d tuanledinh/processedoriginalmonuseg -p /content
!unzip /content/processedoriginalmonuseg.zip -d data

#Importing libraries
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import pywt
from google.colab.patches import cv2_imshow
import math
import tensorflow
import tensorflow as tf
import glob
from PIL import Image
import torch
import torchvision
import torchvision.transforms as transforms
import keras
from keras.models import Model, load_model
from tensorflow.keras.layers import Conv2D, Conv2DTranspose
from keras.layers import concatenate
from keras.layers import Input, BatchNormalization, Activation, UpSampling2D
from tensorflow.keras.layers import Dropout, Lambda
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
from keras import backend as K
from keras.losses import binary_crossentropy
from keras.optimizers import Adam

import cv2
import os

# Define the root directory
root_directory = "/content/data/processed-original-monuseg"

# Lists to store the train images and black and white masks
ImagesTrain = []
BWMTrain = []

# Define the subdirectories containing train images and masks
subdirectories = ["train_folder", "test_folder"]

# Assuming names is a list of file names
for subdirectory in subdirectories:
    img_directory = os.path.join(root_directory, subdirectory, "img")
    mask_directory = os.path.join(root_directory, subdirectory, "labelcol")

    for name in names:
        img_path = os.path.join(img_directory, name + ".png")
        mask_path = os.path.join(mask_directory, name + ".png")

        # Load and append the image
        image = cv2.imread(img_path)
        ImagesTrain.append(image)

        # Load and append the black and white mask
        mask = cv2.imread(mask_path)
        BWMTrain.append(mask)

#Printing to make sure the shapes of the images and masks match, also to check the shape
print(ImagesTrain[1].shape)
print(BWMTrain[1].shape)

#We want to create a larger dataset as 30 ismages is very low.
smalltrain=[]
smalltrainmask=[]
#We will do this by taking 20 random crops of size 256*256 from each training image
for j in range(30):
  for i in range(20):
    img=ImagesTrain[j]
    mask=BWMTrain[j]
    #Generating a arandom position, from where we will crop a window from both the image and its mask so that the mask matches the image
    x = np.random.randint(0, 1000 - 256)
    y = np.random.randint(0, 1000 - 256)
    #Cropping the image and mask at the required positions
    img = img[y:y+256, x:x+256]
    mask = mask[y:y+256, x:x+256]
    #Making a new dataset of smaller images
    smalltrain.append(img)
    smalltrainmask.append(mask)
#Reference: https://blog.roboflow.com/why-and-how-to-implement-random-crop-data-augmentation/

#We want to create a larger dataset as 14 ismages is very low.
smalltest=[]
smalltestmask=[]
#We will do this by taking 20 random crops of size 256*256 from each test image
for j in range(14):
  for i in range(20):
    img=ImagesTest[j]
    mask=BWMTest[j]
    #Generating a arandom position, from where we will crop a window from both the image and its mask so that the mask matches the image
    x = np.random.randint(0, 1000 - 256)
    y = np.random.randint(0, 1000 - 256)
    #Cropping the image and mask at the required positions
    img = img[y:y+256, x:x+256]
    mask = mask[y:y+256, x:x+256]
    #Making a new dataset of smaller images
    smalltest.append(img)
    smalltestmask.append(mask)
#Reference: https://blog.roboflow.com/why-and-how-to-implement-random-crop-data-augmentation/

#Displaying a few cropped training images and their black and white masks
for i in range (3):
    Image,Blackwhite = smalltrain[i],smalltrainmask[i]
    fig,d =  plt.subplots(1,2)
    plt.rcParams["figure.figsize"] = (10,10)

    d[0].imshow(Image, interpolation='nearest')
    d[0].plot()

    d[1].imshow(Blackwhite, interpolation='nearest')
    d[1].plot()

#Displaying a few cropped test images and their black and white masks
for i in range (3):
    Image,Blackwhite = smalltest[i],smalltestmask[i]
    fig,d =  plt.subplots(1,2)
    plt.rcParams["figure.figsize"] = (10,10)

    d[0].imshow(Image, interpolation='nearest')
    d[0].plot()

    d[1].imshow(Blackwhite, interpolation='nearest')
    d[1].plot()

#Printing the number of test and train images in the augmented dataset
print(len(smalltest))
print(len(smalltrain))

#Splitting the test dataset into validation and test (from 280 images to 140 each)
smallval=smalltest[140:280]
smallvalmask=smalltestmask[140:280]
smalltest=smalltest[0:140]
smalltestmask=smalltestmask[0:140]
print(len(smalltest))
print(len(smallval))
print(len(smalltestmask))
print(len(smallvalmask))
#The masks were plot in the same way

!mv "/content/Новая папка_до аугмента" "/content/drive/MyDrive/"

#Making a new normalized dataset,
smalltestnorm=[]
smallvalnorm=[]
smalltrainnorm=[]
#For every test, train and validation image, we normalised the image using min max normalization.
#This helps in training, improves contrast and quality of image and brings the image pixels to values between 0 and 1
for i in range (140):
  smalltestnormimg= cv2.normalize(smalltest[i], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
  smalltestnorm.append(smalltestnormimg)
  #displaying the normalized image vs the original image
  if (i==3):
    Norm,ori = smalltestnorm[i],smalltest[i]
    fig,d =  plt.subplots(1,2)
    plt.rcParams["figure.figsize"] = (10,10)
    d[0].imshow(Norm, interpolation='nearest')
    d[0].plot()
    d[1].imshow(ori, interpolation='nearest')
    d[1].plot()
#For every test, train and validation image, we normalised the image using min max normalization.
#This helps in training, improves contrast and quality of image and brings the image pixels to values between 0 and 1
for i in range (140):
  smallvalnormimg= cv2.normalize(smallval[i], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
  smallvalnorm.append(smallvalnormimg)
  #displaying the normalized image vs the original image
  if (i==3):
    Norm,ori = smallvalnorm[i],smallval[i]
    fig,d =  plt.subplots(1,2)
    plt.rcParams["figure.figsize"] = (10,10)
    d[0].imshow(Norm, interpolation='nearest')
    d[0].plot()
    d[1].imshow(ori, interpolation='nearest')
    d[1].plot()
#For every test, train and validation image, we normalised the image using min max normalization.
#This helps in training, improves contrast and quality of image and brings the image pixels to values between 0 and 1
for i in range (600):
  smalltrainnormimg= cv2.normalize(smalltrain[i], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
  smalltrainnorm.append(smalltrainnormimg)
  #displaying the normalized image vs the original image
  if (i==3):
    Norm,ori = smalltrainnorm[i],smalltrain[i]
    fig,d =  plt.subplots(1,2)
    plt.rcParams["figure.figsize"] = (10,10)
    d[0].imshow(Norm, interpolation='nearest')
    d[0].plot()
    d[1].imshow(ori, interpolation='nearest')
    d[1].plot()
#Reference: https://stackoverflow.com/questions/54666507/fast-image-normalisation-in-python

#Creating a function to perform on the fly flips, rotations and colorjitter augmentations
def Augment(image,mask):
  #Horizonta and vertical flips of both image and mask, with probability half
  if np.random.random() < 0.5:
        image = cv2.flip(image, 1)
        mask = cv2.flip(mask, 1)
  if np.random.random() < 0.5:
        image = cv2.flip(image, 0)
        mask = cv2.flip(mask, 0)
  #Colorjitter augmentations for the network to work better on the H&E stained images
  image=image*255
  image=np.clip(img, a_min = 0, a_max = 255)
  pil_image=Image.fromarray(np.uint8(image))
  #Reference: https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html
  transform = transforms.ColorJitter(brightness=(0.5,1.5), contrast=(0.6,1.6), saturation=(0.7,1.7), hue=(-0.15,0.15))
  new= transform(pil_image)
  img = np.array(new)/255
  img=np.clip(img, a_min = 0, a_max = 255)
  #Multiplied and divided by 255 before and after colorjitter as normalized image has pixel values between 0 and 1
  #Clipped the image to avoid overflow
  return img,mask
  #This function was not used as the data was giving good results without extra augmentations, but it can be used for on the fly augmentations

#Making a diceloss function using its formula for implementation in keras
def dice_loss(y_true, y_pred):
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

#This loss combines Dice loss with the standard binary cross-entropy (BCE) loss that is generally the default for segmentation models.
#Combining the two methods allows for some diversity in the loss, while benefitting from the stability of BCE.
#Dice Loss is widely used in medical image segmentation tasks to address the data imbalance problem.
def bce_dice_loss(y_true, y_pred):
    return binary_crossentropy(y_true, y_pred) + (1 - dice_loss(y_true, y_pred))

"""# Training without data normalization

Initially used the non-normalized data for training the model
"""

#Input image shape provided
inputs = Input((256,256,3))

#Wraps arbitrary expressions as a Layer object.
s = Lambda(lambda x: x / 255) (inputs)

#The downscaling layers have one convolutional layer, then a dropout layer,
#then another convolutional layer and the maxpooling to reduce the size of the image
#Five such blocks
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)
c1 = Dropout(0.1) (c1)
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)
p1 = MaxPooling2D((2, 2)) (c1)

c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)
c2 = Dropout(0.1) (c2)
c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)
p2 = MaxPooling2D((2, 2)) (c2)

c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)
c3 = Dropout(0.1) (c3)
c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)
p3 = MaxPooling2D((2, 2)) (c3)

c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)
c4 = Dropout(0.2) (c4)
c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)
p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)
c5 = Dropout(0.2) (c5)
c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)
p5 = MaxPooling2D(pool_size=(2, 2)) (c5)

#The lowermost layer has two convolutional layers and a dropout layer
c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p5)
c6 = Dropout(0.3) (c6)
c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)

#The upscaling layers have a convolutional layer, then the previous same size output is concatenated(a skip connection),
#then another convolutional layer, a dropout layer and another convolutional layer
#Five such blocks
u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c6)
u7 = concatenate([u7, c5])
c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)
c7 = Dropout(0.2) (c7)
c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)

u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c7)
u8 = concatenate([u8, c4])
c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)
c8 = Dropout(0.2) (c8)
c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)

u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c8)
u9 = concatenate([u9, c3])
c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)
c9 = Dropout(0.1) (c9)
c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)

u10 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c9)
u10 = concatenate([u10, c2])
c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)
c10 = Dropout(0.1) (c10)
c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)

u11 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c10)
u11 = concatenate([u11, c1], axis=3)
c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u11)
c11 = Dropout(0.1) (c11)
c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c11)

#Using the sigmoid activation function in the final layer
outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)

#Compiling the model with adam optimizer, learning rate 0.0003, bce dice loss as the loss and dice loss as the metric
model = Model(inputs=[inputs], outputs=[outputs])
model.compile(optimizer=Adam(lr=0.0003), loss=bce_dice_loss, metrics=[dice_loss])
model.summary()
# Reference: https://towardsdatascience.com/nucleus-segmentation-using-u-net-eceb14a9ced4

#Creating an array of training images and masks
 X_train=np.asarray(smalltrain)
 Y_train=np.asarray(smalltrainmask)
 #Converting the masks to a single channel
 Y_train=Y_train[:,:,:,1]/255

#Creating an array of validation images and masks
 X_val=np.asarray(smallval)
 Y_val=np.asarray(smallvalmask)
#Converting the masks to a single channel
 Y_val=Y_val[:,:,:,1]/255

#Printing and checking the shapes to make sure that they are right to be given to the model
print(X_train.shape)
print(Y_train.shape)
print(X_val.shape)
print(Y_val.shape)

#Early stopper function to stop the epochs if the validation loss does not improve for 5 consecutive epochs
earlystopper = EarlyStopping(monitor="val_loss",patience=5, verbose=1)
#Checkpointer function to save model checkpoints
checkpointer = ModelCheckpoint('msmol.h5', verbose=1, save_best_only=True)
#Fitting the model with the training data, batch size 8 and providing the validation dataset
history=model.fit(X_train,Y_train, batch_size=8, epochs=50, callbacks=[earlystopper,checkpointer],validation_data=(X_val,Y_val))

#Plotting training and validation loss for every epoch
loss_train = history.history['loss']
loss_val = history.history['val_loss']
#Training stopped after 19 epochs
epochs = range(1,20)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
#Reference:https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy

#Saving the test data as a numpy array
X_test=np.asarray(smalltest)
Y_test=np.asarray(smalltestmask)
#Converting test masks to one channel
Y_test=Y_test[:,:,:,1]/255
from skimage.transform import resize
#Predicting the test images
preds_test = model.predict(X_test, verbose=1)
#If the probability value predicted is greater than 0.5, save the value as 1 (round up to integer)
preds_test_t = (preds_test > 0.5).astype(np.uint8)
# Create list of upsampled test masks
preds_test_upsampled = []
for i in range(len(preds_test)):
    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),
                                       (256,256),
                                       mode='constant', preserve_range=True))

!cp -r "/content/drive/MyDrive/Новая папка_до аугмента" /content/

#Printing the test data shapes to check if they are okay
print(X_test.shape)
print(Y_test.shape)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt
i=11
#Plottint the predictions, ground truths and original images in a table to compare
Pred,Ground,Img =preds_test_upsampled[i]*255,smalltestmask[i],smalltestnorm[i]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()
Pred,Ground,Img =preds_test_upsampled[12]*255,smalltestmask[12],smalltestnorm[12]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()

"""Results seem well segmented

Same procedure as before but here we used the data that we had normalized instead of the original colour images
"""

#Input image shape provided
inputs = Input((256,256,3))

#Wraps arbitrary expressions as a Layer object.
s = Lambda(lambda x: x / 255) (inputs)

#The downscaling layers have one convolutional layer, then a dropout layer,
#then another convolutional layer and the maxpooling to reduce the size of the image
#Five such blocks
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)
c1 = Dropout(0.1) (c1)
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)
p1 = MaxPooling2D((2, 2)) (c1)

c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)
c2 = Dropout(0.1) (c2)
c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)
p2 = MaxPooling2D((2, 2)) (c2)

c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)
c3 = Dropout(0.1) (c3)
c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)
p3 = MaxPooling2D((2, 2)) (c3)

c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)
c4 = Dropout(0.2) (c4)
c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)
p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)
c5 = Dropout(0.2) (c5)
c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)
p5 = MaxPooling2D(pool_size=(2, 2)) (c5)

#The lowermost layer has two convolutional layers and a dropout layer
c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p5)
c6 = Dropout(0.3) (c6)
c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)

#The upscaling layers have a convolutional layer, then the previous same size output is concatenated(a skip connection),
#then another convolutional layer, a dropout layer and another convolutional layer
#Five such blocks
u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c6)
u7 = concatenate([u7, c5])
c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)
c7 = Dropout(0.2) (c7)
c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)

u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c7)
u8 = concatenate([u8, c4])
c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)
c8 = Dropout(0.2) (c8)
c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)

u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c8)
u9 = concatenate([u9, c3])
c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)
c9 = Dropout(0.1) (c9)
c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)

u10 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c9)
u10 = concatenate([u10, c2])
c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)
c10 = Dropout(0.1) (c10)
c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)

u11 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c10)
u11 = concatenate([u11, c1], axis=3)
c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u11)
c11 = Dropout(0.1) (c11)
c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c11)

#Using the sigmoid activation function in the final layer
outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)

#Compiling the model with adam optimizer, learning rate 0.0003, bce dice loss as the loss and dice loss as the metric
model = Model(inputs=[inputs], outputs=[outputs])
model.compile(optimizer=Adam(lr=0.0003), loss=bce_dice_loss, metrics=[dice_loss])
model.summary()
# Reference: https://towardsdatascience.com/nucleus-segmentation-using-u-net-eceb14a9ced4

#Using the normalized data for training
X_train=np.asarray(smalltrainnorm)
Y_train=np.asarray(smalltrainmask)
Y_train=Y_train[:,:,:,1]/255

#Using the normalized data for validation
X_val=np.asarray(smallvalnorm)
Y_val=np.asarray(smallvalmask)
Y_val=Y_val[:,:,:,1]/255

#Early stopper function to stop the epochs if the validation loss does not improve for 5 consecutive epochs
earlystopper = EarlyStopping(monitor="val_loss",patience=5, verbose=1)
#Checkpointer function to save model checkpoints
checkpointer = ModelCheckpoint('msmol.h5', verbose=1, save_best_only=True)
#Fitting the model with the training data, batch size 8 and providing the validation dataset
history=model.fit(X_train,Y_train, batch_size=8, epochs=50, callbacks=[earlystopper,checkpointer],validation_data=(X_val,Y_val))

#Plotting training and validation loss for every epoch
loss_train = history.history['loss']
loss_val = history.history['val_loss']
#Model stopped training after 14 epochs
epochs = range(1,15)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
#Reference:https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy

#creating the test arrays from the normalized images
X_test=np.asarray(smalltestnorm)
Y_test=np.asarray(smalltestmask)
#Converting test masks to one channel
Y_test=Y_test[:,:,:,1]/255
from skimage.transform import resize
#Predicting the test images
preds_test = model.predict(X_test, verbose=1)
#If the probability value predicted is greater than 0.5, save the value as 1 (round up to integer)
preds_test_t = (preds_test > 0.5).astype(np.uint8)
# Create list of upsampled test masks
preds_test_upsampled = []
for i in range(len(preds_test)):
    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),
                                       (256,256),
                                       mode='constant', preserve_range=True))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt
i=11
#Plotting the predictions, ground truths and original images in a table to compare
Pred,Ground,Img =preds_test_upsampled[i]*255,smalltestmask[i],smalltestnorm[i]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()
Pred,Ground,Img =preds_test_upsampled[12]*255,smalltestmask[12],smalltestnorm[12]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()

"""This gave a more blurry result than non normalised data.

Not normalizing seems to help give variety to the training images, helping them adapt to staining variability of H&E dye.

Should try colorjitter augmentations

# Training with fewer blocks

Same procedure as before, just by reducing the number of blocks used by the unet from 11 to 9 by removing one downscaling and one upscaling block
"""

#Input image shape provided
inputs = Input((256,256,3))

#Wraps arbitrary expressions as a Layer object.
s = Lambda(lambda x: x / 255) (inputs)

#The downscaling layers have one convolutional layer, then a dropout layer,
#then another convolutional layer and the maxpooling to reduce the size of the image
#Four such blocks
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)
c1 = Dropout(0.1) (c1)
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)
p1 = MaxPooling2D((2, 2)) (c1)

c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)
c2 = Dropout(0.1) (c2)
c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)
p2 = MaxPooling2D((2, 2)) (c2)

c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)
c3 = Dropout(0.2) (c3)
c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)
p3 = MaxPooling2D((2, 2)) (c3)

c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)
c4 = Dropout(0.2) (c4)
c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)
p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

#The lowermost layer has two convolutional layers and a dropout layer
c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)
c5 = Dropout(0.3) (c5)
c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)

#The upscaling layers have a convolutional layer, then the previous same size output is concatenated(a skip connection),
#then another convolutional layer, a dropout layer and another convolutional layer
#Four such blocks
u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)
u6 = concatenate([u6, c4])
c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)
c6 = Dropout(0.2) (c6)
c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)

u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)
u7 = concatenate([u7, c3])
c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)
c7 = Dropout(0.2) (c7)
c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)

u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)
u8 = concatenate([u8, c2])
c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)
c8 = Dropout(0.1) (c8)
c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)

u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)
u9 = concatenate([u9, c1], axis=3)
c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)
c9 = Dropout(0.1) (c9)
c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)

#Using the sigmoid activation function in the final layer
outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)

#Compiling the model with adam optimizer, learning rate 0.0003, bce dice loss as the loss and dice loss as the metric
model = Model(inputs=[inputs], outputs=[outputs])
model.compile(optimizer=Adam(lr=0.0003), loss=bce_dice_loss, metrics=[dice_loss])
model.summary()
# Reference: https://towardsdatascience.com/nucleus-segmentation-using-u-net-eceb14a9ced4

#Creating an array of training images and masks
 X_train=np.asarray(smalltrain)
 Y_train=np.asarray(smalltrainmask)
 #Converting the masks to a single channel
 Y_train=Y_train[:,:,:,1]/255

#Creating an array of validation images and masks
 X_val=np.asarray(smallval)
 Y_val=np.asarray(smallvalmask)
#Converting the masks to a single channel
 Y_val=Y_val[:,:,:,1]/255

#Early stopper function to stop the epochs if the validation loss does not improve for 5 consecutive epochs
earlystopper = EarlyStopping(monitor="val_loss",patience=5, verbose=1)
#Checkpointer function to save model checkpoints
checkpointer = ModelCheckpoint('msmol.h5', verbose=1, save_best_only=True)
#Fitting the model with the training data, batch size 8 and providing the validation dataset
history=model.fit(X_train,Y_train, batch_size=8, epochs=50, callbacks=[earlystopper,checkpointer],validation_data=(X_val,Y_val))

#Plotting training and validation loss for every epoch
loss_train = history.history['loss']
loss_val = history.history['val_loss']
#Training stopped after 14 epochs
epochs = range(1,15)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
#Reference:https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy

#Saving the test data as a numpy array
X_test=np.asarray(smalltest)
Y_test=np.asarray(smalltestmask)
#Converting test masks to one channel
Y_test=Y_test[:,:,:,1]/255
from skimage.transform import resize
#Predicting the test images
preds_test = model.predict(X_test, verbose=1)
#If the probability value predicted is greater than 0.5, save the value as 1 (round up to integer)
preds_test_t = (preds_test > 0.5).astype(np.uint8)
# Create list of upsampled test masks
preds_test_upsampled = []
for i in range(len(preds_test)):
    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),
                                       (256,256),
                                       mode='constant', preserve_range=True))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt
i=11
#Plotting the predictions, ground truths and original images in a table to compare
Pred,Ground,Img =preds_test_upsampled[i]*255,smalltestmask[i],smalltestnorm[i]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()
Pred,Ground,Img =preds_test_upsampled[12]*255,smalltestmask[12],smalltestnorm[12]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()

# tf.image.ssim_multiscale(img1,img2,max_val=255,power_factors=0.1333,filter_size=11,filter_sigma=1.5,k1=0.01,k2=0.03)
#Making a diceloss function using its formula for implementation in keras
def dice_loss(y_true, y_pred):
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def mssimfunc(y_true,y_pred):
  #Means of two images
  y_true_f = K.flatten(y_true)
  y_pred_f = K.flatten(y_pred)
  len=tf.size(y_true_f)
  lent=tf.cast(len, dtype=float)
  xbar, ybar = K.sum(y_true_f)/lent, K.sum(y_pred_f)/lent
  #Variances of the two images
  vara= K.sum((y_true_f-xbar)**2)/(lent - 1)
  varb= K.sum((y_pred_f-ybar)**2)/(lent - 1)
  #Covariance of the two images
  covar= K.sum((y_true_f-xbar)*(y_pred_f-ybar))/(lent - 1)
  ssim= 2*xbar*ybar*2*covar/((xbar**2+ybar**2)*(vara+varb))
  return ssim

#This loss combines Dice loss with the msssim loss that is generally the default for segmentation models.
def MSSIM_dice_loss(y_true, y_pred,a=0.5):
    return a*(1-mssimfunc(y_true,y_pred))+ (1-a)*(1 - dice_loss(y_true, y_pred))
# Reference: https://stackoverflow.com/questions/65013894/trade-off-between-losses

#Input image shape provided
inputs = Input((256,256,3))

#Wraps arbitrary expressions as a Layer object.
s = Lambda(lambda x: x / 255) (inputs)

#The downscaling layers have one convolutional layer, then a dropout layer,
#then another convolutional layer and the maxpooling to reduce the size of the image
#Five such blocks
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)
c1 = Dropout(0.1) (c1)
c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)
p1 = MaxPooling2D((2, 2)) (c1)

c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)
c2 = Dropout(0.1) (c2)
c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)
p2 = MaxPooling2D((2, 2)) (c2)

c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)
c3 = Dropout(0.1) (c3)
c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)
p3 = MaxPooling2D((2, 2)) (c3)

c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)
c4 = Dropout(0.2) (c4)
c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)
p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)
c5 = Dropout(0.2) (c5)
c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)
p5 = MaxPooling2D(pool_size=(2, 2)) (c5)

#The lowermost layer has two convolutional layers and a dropout layer
c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p5)
c6 = Dropout(0.3) (c6)
c6 = Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)

#The upscaling layers have a convolutional layer, then the previous same size output is concatenated(a skip connection),
#then another convolutional layer, a dropout layer and another convolutional layer
#Five such blocks
u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (c6)
u7 = concatenate([u7, c5])
c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)
c7 = Dropout(0.2) (c7)
c7 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)

u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c7)
u8 = concatenate([u8, c4])
c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)
c8 = Dropout(0.2) (c8)
c8 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)

u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c8)
u9 = concatenate([u9, c3])
c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)
c9 = Dropout(0.1) (c9)
c9 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)

u10 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c9)
u10 = concatenate([u10, c2])
c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)
c10 = Dropout(0.1) (c10)
c10 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)

u11 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c10)
u11 = concatenate([u11, c1], axis=3)
c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u11)
c11 = Dropout(0.1) (c11)
c11 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c11)

#Using the sigmoid activation function in the final layer
outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)

#Compiling the model with adam optimizer, learning rate 0.0003, mssim dice loss as the loss and dice loss as the metric
model = Model(inputs=[inputs], outputs=[outputs])
model.compile(optimizer=Adam(lr=0.0003), loss=MSSIM_dice_loss, metrics=[dice_loss])
model.summary()
# Reference: https://towardsdatascience.com/nucleus-segmentation-using-u-net-eceb14a9ced4

#Creating an array of training images and masks
 X_train=np.asarray(smalltrain)
 Y_train=np.asarray(smalltrainmask)
 #Converting the masks to a single channel
 Y_train=Y_train[:,:,:,1]/255

#Creating an array of validation images and masks
 X_val=np.asarray(smallval)
 Y_val=np.asarray(smallvalmask)
#Converting the masks to a single channel
 Y_val=Y_val[:,:,:,1]/255

#Early stopper function to stop the epochs if the validation loss does not improve for 5 consecutive epochs
earlystopper = EarlyStopping(monitor="val_loss",patience=5, verbose=1)
#Checkpointer function to save model checkpoints
checkpointer = ModelCheckpoint('msmol.h5', verbose=1, save_best_only=True)
#Fitting the model with the training data, batch size 8 and providing the validation dataset
history=model.fit(X_train,Y_train, batch_size=8, epochs=50, callbacks=[earlystopper,checkpointer],validation_data=(X_val,Y_val))

#Plotting training and validation loss for every epoch
loss_train = history.history['loss']
loss_val = history.history['val_loss']
#Training stopped after 35 epochs
epochs = range(1,36)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
#Reference:https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy

#Saving the test data as a numpy array
X_test=np.asarray(smalltest)
Y_test=np.asarray(smalltestmask)
#Converting test masks to one channel
Y_test=Y_test[:,:,:,1]/255
from skimage.transform import resize
#Predicting the test images
preds_test = model.predict(X_test, verbose=1)
#If the probability value predicted is greater than 0.5, save the value as 1 (round up to integer)
preds_test_t = (preds_test > 0.5).astype(np.uint8)
# Create list of upsampled test masks
preds_test_upsampled = []
for i in range(len(preds_test)):
    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),
                                       (256,256),
                                       mode='constant', preserve_range=True))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt
i=11
#Plotting the predictions, ground truths and original images in a table to compare
Pred,Ground,Img =preds_test_upsampled[i]*255,smalltestmask[i],smalltestnorm[i]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()
Pred,Ground,Img =preds_test_upsampled[12]*255,smalltestmask[12],smalltestnorm[12]
fig,d =  plt.subplots(1,3)
plt.rcParams["figure.figsize"] = (15,15)
d[0].imshow(Pred, interpolation='nearest')
d[0].plot()
d[1].imshow(Ground, interpolation='nearest')
d[1].plot()
d[2].imshow(Img, interpolation='nearest')
d[2].plot()

"""MSSIM loss with dice loss gave crisp edges, and reduced the loss at a good rate leading to training for longer.

However, some of the nuclei have been clubbed into one

# Watershed Segmentation
"""

#Reference: https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html
def watersheding(image):
  img=image.copy()
  #The cv2 functions used below require uint8 input (binary masks)
  img = img.astype("uint8")
  ret, thresh = cv2.threshold(img,0,255,cv2.THRESH_BINARY)
  ## Defining kernel for opening operation
  kernel = np.ones((3,3), np.uint8)
  #Opening the binary mask
  opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)
  ## After opening, will perform dilation
  sure_bg = cv2.dilate(opening, kernel, iterations=3)
  ## Sure background image
  opening = cv2.cvtColor(opening, cv2.COLOR_BGR2GRAY)
  sure_bg = cv2.cvtColor(sure_bg, cv2.COLOR_BGR2GRAY)
  #The cv2 functions used also require images to be in a single channel
  opening = opening.astype("uint8")
  #After opening we will also get the distance transform of the opened image
  dist_transform = cv2.distanceTransform(opening,distanceType=cv2.DIST_L2,maskSize=5)
  #Thresholding this distance transform again to get a sure foreground
  ret, sure_fg = cv2.threshold(dist_transform,0.2*dist_transform.max(),255,0)
  sure_fg = np.uint8(sure_fg)
  ## Sure foreground image
  minus_img = np.subtract(sure_bg, sure_fg)
  #The difference between the sure bg and sure fg
  ret, markers = cv2.connectedComponents(sure_fg)
  #generating markers to use in the watershed function
  markers = markers +1
  ## Add one so that sure background is not 1
  markers[minus_img == 255] = 0
  ## Making the unknown area as 0
  markers = cv2.watershed(img, markers)
  #Watershed segmentation
  img[markers == -1] = (255, 0, 0)
  ## boundary region is marked with -1
  return img, opening, sure_bg, minus_img
  #returning the sure bg, sure fg and their difference  and the watershed segmentation results

lista=[]
listb=[]
listc=[]
listd=[]
#Making lists to store the watershed segmented image, the sure foreground, sure background and their difference
for i in range (140):
  im=np.zeros((256,256,3),float)
  #Converting the predicted image to a 3 channel image as that is what the watershed function takes as input
  im[:,:,0]=preds_test_upsampled[i]
  im[:,:,1]=preds_test_upsampled[i]
  im[:,:,2]=preds_test_upsampled[i]
  lista.append(watersheding(im)[0])
  listb.append(watersheding(im)[1])
  listc.append(watersheding(im)[2])
  listd.append(watersheding(im)[3])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt
#Making a plot which shows the watershed segmented image, the sure foreground, sure background and their difference for a few images
for i in range (4,10):
  a,b,c,d= lista[i],listb[i],listc[i],listd[i]
  fig,d =  plt.subplots(1,5)
  plt.rcParams["figure.figsize"] = (18,18)
  d[0].imshow(X_test[i], interpolation='nearest')
  d[0].plot()
  d[1].imshow(a, interpolation='nearest')
  d[1].plot()
  d[2].imshow(b, interpolation='nearest')
  d[2].plot()
  d[3].imshow(c, interpolation='nearest')
  d[3].plot()
  d[4].imshow(c-b, interpolation='nearest')
  d[4].plot()

"""# References:

For the glob library: https://pynative.com/python-glob#:~:text=The%20glob%20module%2C%20part%20of,Unix%20Shell%20path%20expansion%20rules

For matplotlib subplots: https://www.w3schools.com/python/matplotlib_subplot.asp

For random crop data augmentations: https://blog.roboflow.com/why-and-how-to-implement-random-crop-data-augmentation/

For image normalization: https://stackoverflow.com/questions/54666507/fast-image-normalisation-in-python

For colorjitter augmentation: https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html

For the UNET model: https://towardsdatascience.com/nucleus-segmentation-using-u-net-eceb14a9ced4

For plotting the train and validation losses: https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy

For mssim tradeoff: https://stackoverflow.com/questions/65013894/trade-off-between-losses

For watershed segmentation: https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html

# Alternative tries
"""

def dice_loss(y_true, y_pred):
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

# def gaussian(x, mu, sigma):
#     return np.exp(-(float(x) - float(mu)) ** 2 / (2 * sigma ** 2))


# def make_kernel(sigma):
#     # kernel radius = 2*sigma, but minimum 3x3 matrix
#     kernel_size = max(3, int(2 * 2 * sigma + 1))
#     mean = np.floor(0.5 * kernel_size)
#     kernel_1d = np.array([gaussian(x, mean, sigma) for x in range(kernel_size)])
#     # make 2D kernel
#     np_kernel = np.outer(kernel_1d, kernel_1d).astype(dtype=K.floatx())
#     # normalize kernel by sum of elements
#     kernel = np_kernel / np.sum(np_kernel)
#     kernel = np.reshape(kernel, (kernel_size, kernel_size, 1,1))    #height, width, in_channels, out_channel
#     return kernel

# def keras_SSIM_cs(y_true, y_pred):
#     axis=None
#     gaussian = make_kernel(1.5)
#     x = tf.nn.conv2d(y_true, gaussian, strides=[1, 1, 1, 1], padding='SAME')
#     y = tf.nn.conv2d(y_pred, gaussian, strides=[1, 1, 1, 1], padding='SAME')

#     u_x=K.mean(x, axis=axis)
#     u_y=K.mean(y, axis=axis)

#     var_x=K.var(x, axis=axis)
#     var_y=K.var(y, axis=axis)

#     cov_xy=K.cov(x, y, axis)

#     K1=0.01
#     K2=0.03
#     L=1  # depth of image (255 in case the image has a differnt scale)

#     C1=(K1*L)**2
#     C2=(K2*L)**2
#     C3=C2/2

#     l = ((2*u_x*u_y)+C1) / (K.pow(u_x,2) + K.pow(u_x,2) + C1)
#     c = ((2*K.sqrt(var_x)*K.sqrt(var_y))+C2) / (var_x + var_y + C2)
#     s = (cov_xy+C3) / (K.sqrt(var_x)*K.sqrt(var_y) + C3)

#     return [c,s,l]

# def keras_MS_SSIM(y_true, y_pred):
#     iterations = 5
#     x=y_true
#     y=y_pred
#     weight = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]
#     c=[]
#     s=[]
#     for i in range(iterations):
#         cs=keras_SSIM_cs(x, y)
#         c.append(cs[0])
#         s.append(cs[1])
#         l=cs[2]
#         if(i!=4):
#             x=tf.image.resize_images(x, (x.get_shape().as_list()[1]//(2**(i+1)), x.get_shape().as_list()[2]//(2**(i+1))))
#             y=tf.image.resize_images(y, (y.get_shape().as_list()[1]//(2**(i+1)), y.get_shape().as_list()[2]//(2**(i+1))))
#     c = tf.stack(c)
#     s = tf.stack(s)
#     cs = c*s

#     #Normalize: suggestion from https://github.com/jorge-pessoa/pytorch-msssim/issues/2 last comment to avoid NaN values
#     l=(l+1)/2
#     cs=(cs+1)/2

#     cs=cs**weight
#     cs = tf.reduce_prod(cs)
#     l=l**weight[-1]

#     ms_ssim = l*cs
#     ms_ssim = tf.where(tf.is_nan(ms_ssim), K.zeros_like(ms_ssim), ms_ssim)

#     return K.mean(ms_ssim)


def mssimfunc(y_true,y_pred):
  # print(y_true.shape)
  # print(y_pred.shape)

  return tf.image.ssim_multiscale(y_true,y_pred,max_val=255.0, filter_size=2,filter_sigma=0)


def MSSIM_dice_loss(y_true, y_pred):
    return (1-mssimfunc(y_true,y_pred))+ (1 - dice_loss(y_true, y_pred))